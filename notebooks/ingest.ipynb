{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d55e397",
   "metadata": {},
   "source": [
    "## VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d578d05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae3e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 获取当前 notebook 的工作目录（通常是 .ipynb 所在目录）\n",
    "notebook_dir = os.getcwd()\n",
    "target_path = os.path.abspath(os.path.join(notebook_dir, \"../\"))\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "\n",
    "from src.vectorstore import get_vector_store\n",
    "from src.configuration import Configuration\n",
    "from src.embeddings import get_embeddings_model\n",
    "\n",
    "collection_name = \"langchain\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17715b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the collection:  32038\n"
     ]
    }
   ],
   "source": [
    "config = Configuration()\n",
    "embedding = get_embeddings_model(config.embedding_model)\n",
    "store = get_vector_store(\n",
    "    provider=config.retriever_provider,\n",
    "    storage_type=config.storage_type,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")\n",
    "collection = store._collection\n",
    "print(\"Number of vectors in the collection: \", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fcd3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain\n",
      "test_collection\n"
     ]
    }
   ],
   "source": [
    "collections = store._client.list_collections()\n",
    "for collection in collections:\n",
    "    print(collection.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501e7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = store.similarity_search_with_score(\n",
    "    \"return_uuids from Weaviate\", k=1, filter={\"type\": \"code\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a877b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e12ff880-c500-5114-865e-06c3c865b703',\n",
       " 'metadata': {'title': 'Source code for langchain_community.retrievers.weaviate_hybrid_search',\n",
       "  'type': 'code',\n",
       "  'source': 'https://python.langchain.com/api_reference/_modules/langchain_community/retrievers/weaviate_hybrid_search.html',\n",
       "  'lang': 'python'},\n",
       " 'page_content': '# Source code for langchain_community.retrievers.weaviate_hybrid_search\\n\\n```\\n\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional, cast\\nfrom uuid import uuid4\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\\nfrom langchain_core.documents import Document\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom pydantic import ConfigDict, model_validator\\n\\n@deprecated(\\n    since=\"0.3.18\",\\n    removal=\"1.0\",\\n    alternative_import=\"langchain_weaviate.WeaviateVectorStore\",\\n)\\nclass WeaviateHybridSearchRetriever(BaseRetriever):\\n    \"\"\"`Weaviate hybrid search` retriever.\\n\\n    See the documentation:\\n      https://weaviate.io/blog/hybrid-search-explained\\n    \"\"\"\\n\\n    client: Any = None\\n    \"\"\"keyword arguments to pass to the Weaviate client.\"\"\"\\n    index_name: str\\n    \"\"\"The name of the index to use.\"\"\"\\n    text_key: str\\n    \"\"\"The name of the text key to use.\"\"\"\\n    alpha: float = 0.5\\n    \"\"\"The weight of the text key in the hybrid search.\"\"\"\\n    k: int = 4\\n    \"\"\"The number of results to return.\"\"\"\\n    attributes: List[str]\\n    \"\"\"The attributes to return in the results.\"\"\"\\n    create_schema_if_missing: bool = True\\n    \"\"\"Whether to create the schema if it doesn\\'t exist.\"\"\"\\n\\n    @model_validator(mode=\"before\")\\n    @classmethod\\n    def validate_client(\\n        cls,\\n        values: Dict[str, Any],\\n    ) -> Any:\\n        try:\\n            import weaviate\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import weaviate python package. \"\\n                \"Please install it with `pip install weaviate-client`.\"\\n            )\\n        if not isinstance(values[\"client\"], weaviate.Client):\\n            client = values[\"client\"]\\n            raise ValueError(\\n                f\"client should be an instance of weaviate.Client, got {type(client)}\"\\n            )\\n        if values.get(\"attributes\") is None:\\n            values[\"attributes\"] = []\\n\\n        cast(List, values[\"attributes\"]).append(values[\"text_key\"])\\n\\n        if values.get(\"create_schema_if_missing\", True):\\n            class_obj = {\\n                \"class\": values[\"index_name\"],\\n                \"properties\": [{\"name\": values[\"text_key\"], \"dataType\": [\"text\"]}],\\n                \"vectorizer\": \"text2vec-openai\",\\n            }\\n\\n            if not values[\"client\"].schema.exists(values[\"index_name\"]):\\n                values[\"client\"].schema.create_class(class_obj)\\n\\n        return values\\n\\n    model_config = ConfigDict(\\n        arbitrary_types_allowed=True,\\n    )\\n\\n    # added text_key\\n\\n    def add_documents(self, docs: List[Document], **kwargs: Any) -> List[str]:\\n        \"\"\"Upload documents to Weaviate.\"\"\"\\n        from weaviate.util import get_valid_uuid\\n\\n        with self.client.batch as batch:\\n            ids = []\\n            for i, doc in enumerate(docs):\\n                metadata = doc.metadata or {}\\n                data_properties = {self.text_key: doc.page_content, **metadata}\\n\\n                # If the UUID of one of the objects already exists\\n                # then the existing objectwill be replaced by the new object.\\n                if \"uuids\" in kwargs:\\n                    _id = kwargs[\"uuids\"][i]\\n                else:\\n                    _id = get_valid_uuid(uuid4())\\n\\n                batch.add_data_object(data_properties, self.index_name, _id)\\n                ids.append(_id)\\n        return ids\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n        where_filter: Optional[Dict[str, object]] = None,\\n        score: bool = False,\\n        hybrid_search_kwargs: Optional[Dict[str, object]] = None,\\n    ) -> List[Document]:\\n        \"\"\"Look up similar documents in Weaviate.\\n\\n        query: The query to search for relevant documents\\n         of using weviate hybrid search.',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "\n",
    "\n",
    "doc: Document = results[0][0]\n",
    "doc.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f54c4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'e12ff880-c500-5114-865e-06c3c865b703'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'e12ff880-c500-5114-865e-06c3c865b703'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Similarity: 1.072657'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Similarity: 1.072657'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Source code for langchain_community.retrievers.weaviate_hybrid_search'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'code'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://python.langchain.com/api_reference/_modules/langchain_community/retrievers/weaviate_hybrid_search.html'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'lang'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'python'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Source code for langchain_community.retrievers.weaviate_hybrid_search'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'code'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://python.langchain.com/api_reference/_modules/langchain_community/retrievers/weaviate_hybrid_search.html'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'lang'\u001b[0m: \u001b[32m'python'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                      <span style=\"font-weight: bold\">Source code for langchain_community.retrievers.weaviate_hybrid_search</span>                      ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from __future__ import annotations</span><span style=\"background-color: #272822\">                                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from typing import Any, Dict, List, Optional, cast</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from uuid import uuid4</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from langchain_core._api import deprecated</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from langchain_core.callbacks import CallbackManagerForRetrieverRun</span><span style=\"background-color: #272822\">                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from langchain_core.documents import Document</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from langchain_core.retrievers import BaseRetriever</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from pydantic import ConfigDict, model_validator</span><span style=\"background-color: #272822\">                                                                  </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">@deprecated(</span><span style=\"background-color: #272822\">                                                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    since=\"0.3.18\",</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    removal=\"1.0\",</span><span style=\"background-color: #272822\">                                                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    alternative_import=\"langchain_weaviate.WeaviateVectorStore\",</span><span style=\"background-color: #272822\">                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                 </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">class WeaviateHybridSearchRetriever(BaseRetriever):</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"`Weaviate hybrid search` retriever.</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    See the documentation:</span><span style=\"background-color: #272822\">                                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">      https://weaviate.io/blog/hybrid-search-explained</span><span style=\"background-color: #272822\">                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"</span><span style=\"background-color: #272822\">                                                                                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    client: Any = None</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"keyword arguments to pass to the Weaviate client.\"\"\"</span><span style=\"background-color: #272822\">                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    index_name: str</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"The name of the index to use.\"\"\"</span><span style=\"background-color: #272822\">                                                                           </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    text_key: str</span><span style=\"background-color: #272822\">                                                                                                 </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"The name of the text key to use.\"\"\"</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    alpha: float = 0.5</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"The weight of the text key in the hybrid search.\"\"\"</span><span style=\"background-color: #272822\">                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    k: int = 4</span><span style=\"background-color: #272822\">                                                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"The number of results to return.\"\"\"</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    attributes: List[str]</span><span style=\"background-color: #272822\">                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"The attributes to return in the results.\"\"\"</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    create_schema_if_missing: bool = True</span><span style=\"background-color: #272822\">                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    \"\"\"Whether to create the schema if it doesn't exist.\"\"\"</span><span style=\"background-color: #272822\">                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    @model_validator(mode=\"before\")</span><span style=\"background-color: #272822\">                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    @classmethod</span><span style=\"background-color: #272822\">                                                                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    def validate_client(</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        cls,</span><span style=\"background-color: #272822\">                                                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        values: Dict[str, Any],</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    ) -&gt; Any:</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        try:</span><span style=\"background-color: #272822\">                                                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            import weaviate</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        except ImportError:</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            raise ImportError(</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                \"Could not import weaviate python package. \"</span><span style=\"background-color: #272822\">                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                \"Please install it with `pip install weaviate-client`.\"</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            )</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        if not isinstance(values[\"client\"], weaviate.Client):</span><span style=\"background-color: #272822\">                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            client = values[\"client\"]</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            raise ValueError(</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                f\"client should be an instance of weaviate.Client, got {type(client)}\"</span><span style=\"background-color: #272822\">                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            )</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        if values.get(\"attributes\") is None:</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            values[\"attributes\"] = []</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        cast(List, values[\"attributes\"]).append(values[\"text_key\"])</span><span style=\"background-color: #272822\">                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        if values.get(\"create_schema_if_missing\", True):</span><span style=\"background-color: #272822\">                                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            class_obj = {</span><span style=\"background-color: #272822\">                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                \"class\": values[\"index_name\"],</span><span style=\"background-color: #272822\">                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                \"properties\": [{\"name\": values[\"text_key\"], \"dataType\": [\"text\"]}],</span><span style=\"background-color: #272822\">                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                \"vectorizer\": \"text2vec-openai\",</span><span style=\"background-color: #272822\">                                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            }</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            if not values[\"client\"].schema.exists(values[\"index_name\"]):</span><span style=\"background-color: #272822\">                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                values[\"client\"].schema.create_class(class_obj)</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        return values</span><span style=\"background-color: #272822\">                                                                                             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    model_config = ConfigDict(</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        arbitrary_types_allowed=True,</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    )</span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # added text_key</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    def add_documents(self, docs: List[Document], **kwargs: Any) -&gt; List[str]:</span><span style=\"background-color: #272822\">                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        \"\"\"Upload documents to Weaviate.\"\"\"</span><span style=\"background-color: #272822\">                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        from weaviate.util import get_valid_uuid</span><span style=\"background-color: #272822\">                                                                  </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        with self.client.batch as batch:</span><span style=\"background-color: #272822\">                                                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            ids = []</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            for i, doc in enumerate(docs):</span><span style=\"background-color: #272822\">                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                metadata = doc.metadata or {}</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                data_properties = {self.text_key: doc.page_content, **metadata}</span><span style=\"background-color: #272822\">                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                # If the UUID of one of the objects already exists</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                # then the existing objectwill be replaced by the new object.</span><span style=\"background-color: #272822\">                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                if \"uuids\" in kwargs:</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                    _id = kwargs[\"uuids\"][i]</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                else:</span><span style=\"background-color: #272822\">                                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                    _id = get_valid_uuid(uuid4())</span><span style=\"background-color: #272822\">                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                batch.add_data_object(data_properties, self.index_name, _id)</span><span style=\"background-color: #272822\">                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                ids.append(_id)</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        return ids</span><span style=\"background-color: #272822\">                                                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    def _get_relevant_documents(</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        self,</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        query: str,</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        *,</span><span style=\"background-color: #272822\">                                                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        run_manager: CallbackManagerForRetrieverRun,</span><span style=\"background-color: #272822\">                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        where_filter: Optional[Dict[str, object]] = None,</span><span style=\"background-color: #272822\">                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        score: bool = False,</span><span style=\"background-color: #272822\">                                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        hybrid_search_kwargs: Optional[Dict[str, object]] = None,</span><span style=\"background-color: #272822\">                                                 </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    ) -&gt; List[Document]:</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        \"\"\"Look up similar documents in Weaviate.</span><span style=\"background-color: #272822\">                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        query: The query to search for relevant documents</span><span style=\"background-color: #272822\">                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">         of using weviate hybrid search.</span><span style=\"background-color: #272822\">                                                                          </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                      \u001b[1mSource code for langchain_community.retrievers.weaviate_hybrid_search\u001b[0m                      ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom __future__ import annotations\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom typing import Any, Dict, List, Optional, cast\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom uuid import uuid4\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom langchain_core._api import deprecated\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom langchain_core.documents import Document\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom langchain_core.retrievers import BaseRetriever\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom pydantic import ConfigDict, model_validator\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m@deprecated(\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    since=\"0.3.18\",\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    removal=\"1.0\",\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    alternative_import=\"langchain_weaviate.WeaviateVectorStore\",\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mclass WeaviateHybridSearchRetriever(BaseRetriever):\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"`Weaviate hybrid search` retriever.\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    See the documentation:\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m      https://weaviate.io/blog/hybrid-search-explained\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    client: Any = None\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"keyword arguments to pass to the Weaviate client.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    index_name: str\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"The name of the index to use.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    text_key: str\u001b[0m\u001b[48;2;39;40;34m                                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"The name of the text key to use.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    alpha: float = 0.5\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"The weight of the text key in the hybrid search.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    k: int = 4\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"The number of results to return.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    attributes: List[str]\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"The attributes to return in the results.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    create_schema_if_missing: bool = True\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \"\"\"Whether to create the schema if it doesn't exist.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    @model_validator(mode=\"before\")\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    @classmethod\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    def validate_client(\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        cls,\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        values: Dict[str, Any],\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    ) -> Any:\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        try:\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            import weaviate\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        except ImportError:\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            raise ImportError(\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                \"Could not import weaviate python package. \"\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                \"Please install it with `pip install weaviate-client`.\"\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            )\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        if not isinstance(values[\"client\"], weaviate.Client):\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            client = values[\"client\"]\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            raise ValueError(\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                f\"client should be an instance of weaviate.Client, got {type(client)}\"\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            )\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        if values.get(\"attributes\") is None:\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            values[\"attributes\"] = []\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        cast(List, values[\"attributes\"]).append(values[\"text_key\"])\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        if values.get(\"create_schema_if_missing\", True):\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            class_obj = {\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                \"class\": values[\"index_name\"],\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                \"properties\": [{\"name\": values[\"text_key\"], \"dataType\": [\"text\"]}],\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                \"vectorizer\": \"text2vec-openai\",\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            }\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            if not values[\"client\"].schema.exists(values[\"index_name\"]):\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                values[\"client\"].schema.create_class(class_obj)\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        return values\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    model_config = ConfigDict(\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        arbitrary_types_allowed=True,\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    )\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    # added text_key\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    def add_documents(self, docs: List[Document], **kwargs: Any) -> List[str]:\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \"\"\"Upload documents to Weaviate.\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        from weaviate.util import get_valid_uuid\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        with self.client.batch as batch:\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            ids = []\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m            for i, doc in enumerate(docs):\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                metadata = doc.metadata or {}\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                data_properties = {self.text_key: doc.page_content, **metadata}\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                # If the UUID of one of the objects already exists\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                # then the existing objectwill be replaced by the new object.\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                if \"uuids\" in kwargs:\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                    _id = kwargs[\"uuids\"][i]\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                else:\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                    _id = get_valid_uuid(uuid4())\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                batch.add_data_object(data_properties, self.index_name, _id)\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m                ids.append(_id)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        return ids\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    def _get_relevant_documents(\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        self,\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        query: str,\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        *,\u001b[0m\u001b[48;2;39;40;34m                                                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        run_manager: CallbackManagerForRetrieverRun,\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        where_filter: Optional[Dict[str, object]] = None,\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        score: bool = False,\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        hybrid_search_kwargs: Optional[Dict[str, object]] = None,\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    ) -> List[Document]:\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        \"\"\"Look up similar documents in Weaviate.\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        query: The query to search for relevant documents\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m         of using weviate hybrid search.\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.pretty import pprint\n",
    "\n",
    "for res, score in results[0:1]:\n",
    "    pprint(res.id)\n",
    "    pprint(f\"Similarity: {score:3f}\")\n",
    "    pprint(res.metadata)\n",
    "    retrieved_md = Markdown(res.page_content)\n",
    "    Console().print(retrieved_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1201f",
   "metadata": {},
   "source": [
    "## Langchain graph smith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960b95c",
   "metadata": {},
   "source": [
    "### LangGraph Loader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe73a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://langchain-ai.github.io/langgraph/reference/graphs/', 'title': 'Graphs', 'type': 'api', 'lang': 'python'}\n",
      "Table of contents\n",
      "- [class StateGraph](#langgraph.graph.state.StateGraph)\n",
      "  - [meth add_node](#langgraph.graph.state.StateGraph.add_node)\n",
      "  - [meth add_edge](#langgraph.graph.state.StateGraph.add_edge)\n",
      "  - [meth add_conditional_edges](#langgraph.graph.state.StateGraph.add_conditional_edges)\n",
      "  - [meth add_sequence](#langgraph.graph.state.StateGraph.add_sequence)\n",
      "  - [meth compile](#langgraph.graph.state.StateGraph.compile)\n",
      "- [class CompiledStateGraph](#langgraph.graph.state.CompiledStateGraph)\n",
      "  - [meth stream](#langgraph.graph.state.CompiledStateGraph.stream)\n",
      "  - [meth astream](#langgraph.graph.state.CompiledStateGraph.astream)\n",
      "  - [meth invoke](#langgraph.graph.state.CompiledStateGraph.invoke)\n",
      "  - [meth ainvoke](#langgraph.graph.state.CompiledStateGraph.ainvoke)\n",
      "  - [meth get_state](#langgraph.graph.state.CompiledStateGraph.get_state)\n",
      "  - [meth aget_state](#langgraph.graph.state.CompiledStateGraph.aget_state)\n",
      "  - [meth get_state_history](#langgraph.graph.state.CompiledStateGraph.get_state_history)\n",
      "  - [meth aget_state_history](#langgraph.graph.state.CompiledStateGraph.aget_state_history)\n",
      "  - [meth update_state](#langgraph.graph.state.CompiledStateGraph.update_state)\n",
      "  - [meth aupdate_state](#langgraph.graph.state.CompiledStateGraph.aupdate_state)\n",
      "  - [meth bulk_update_state](#langgraph.graph.state.CompiledStateGraph.bulk_update_state)\n",
      "  - [meth abulk_update_state](#langgraph.graph.state.CompiledStateGraph.abulk_update_state)\n",
      "  - [meth get_graph](#langgraph.graph.state.CompiledStateGraph.get_graph)\n",
      "  - [meth aget_graph](#langgraph.graph.state.CompiledStateGraph.aget_graph)\n",
      "  - [meth get_subgraphs](#langgraph.graph.state.CompiledStateGraph.get_subgraphs)\n",
      "  - [meth aget_subgraphs](#langgraph.graph.state.CompiledStateGraph.aget_subgraphs)\n",
      "  - [meth with_config](#langgraph.graph.state.CompiledStateGraph.with_config)\n",
      "- [func add_messages](#langgraph.graph.message.add_messages)\n",
      "\n",
      "[\n",
      "\n",
      "](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/reference/graphs.md)\n",
      "# [Graph Definition](#graph-definitions)\n",
      "\n",
      "## [StateGrap](#langgraph.graph.state.StateGraph)\n",
      "\n",
      "              Bases: `Generic[StateT, ContextT, InputT, OutputT]`\n",
      "\n",
      "A graph whose nodes communicate by reading and writing to a shared state.\n",
      "The signature of each node is State -> Partial.\n",
      "\n",
      "Each state key can optionally be annotated with a reducer function that\n",
      "will be used to aggregate the values of that key received from multiple nodes.\n",
      "The signature of a reducer function is (Value, Value) -> Value.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| state_schema | type[StateT] | The schema class that defines the state. | required |\n",
      "| context_schema | type[ContextT] | None | The schema class that defines the runtime context.\n",
      "Use this to expose immutable context data to your nodes, like user_id, db_conn, etc. | None |\n",
      "| input_schema | type[InputT] | None | The schema class that defines the input to the graph. | None |\n",
      "| output_schema | type[OutputT] | None | The schema class that defines the output from the graph. | None |\n",
      "\n",
      "`config_schema` Deprecated\n",
      "\n",
      "The `config_schema` parameter is deprecated in v0.6.0 and support will be removed in v2.0.0.\n",
      "Please use `context_schema` instead to specify the schema for run-scoped context.\n",
      "\n",
      "Example\n",
      "```python\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from typing_extensions import Annotated, TypedDict\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langgraph.graph import StateGraph\n",
      "from langgraph.runtime import Runtime\n",
      "\n",
      "def reducer(a: list, b: int | None) -> list:\n",
      "    if b is not None:\n",
      "        return a + [b]\n",
      "    return a\n",
      "\n",
      "class State(TypedDict):\n",
      "    x: Annotated[list, reducer]\n",
      "\n",
      "class Context(TypedDict):\n",
      "    r: float\n",
      "\n",
      "graph = StateGraph(state_schema=State, context_schema=Context)\n",
      "\n",
      "def node(state: State, runtime: Runtime[Context]) -> dict:\n",
      "    r = runtie.context.get(\"r\", 1.0)\n",
      "    x = state[\"x\"][-1]\n",
      "    next_value = x * r * (1 - x)\n",
      "    return {\"x\": next_value}\n",
      "\n",
      "graph.add_node(\"A\", node)\n",
      "graph.set_entry_point(\"A\")\n",
      "graph.set_finish_point(\"A\")\n",
      "compiled = graph.compile()\n",
      "\n",
      "step1 = compiled.invoke({\"x\": 0.5}, context={\"r\": 3.0})\n",
      "# {'x': [0.5, 0.75]}\n",
      "\n",
      "```\n",
      "\n",
      "Methods:\n",
      "\n",
      "| Name | Description |\n",
      "| ---- | ---- |\n",
      "| add_node | Add a new node to the state graph. |\n",
      "| add_edge | Add a directed edge from the start node (or list of start nodes) to the end node. |\n",
      "| add_conditional_edges | Add a conditional edge from the starting node to any number of destination nodes. |\n",
      "| add_sequence | Add a sequence of nodes that will be executed in the provided order. |\n",
      "| compile | Compiles the state graph into aCompiledStateGraphobject. |\n",
      "\n",
      "### [add_nod](#langgraph.graph.state.StateGraph.add_node)\n",
      "\n",
      "```python\n",
      "add_node(\n",
      "    node: str | StateNode[NodeInputT, ContextT],\n",
      "    action: StateNode[NodeInputT, ContextT] | None = None,\n",
      "    *,\n",
      "    defer: bool = False,\n",
      "    metadata: dict[str, Any] | None = None,\n",
      "    input_schema: type[NodeInputT] | None = None,\n",
      "    retry_policy: (\n",
      "        RetryPolicy | Sequence[RetryPolicy] | None\n",
      "    ) = None,\n",
      "    cache_policy: CachePolicy | None = None,\n",
      "    destinations: (\n",
      "        dict[str, str] | tuple[str, ...] | None\n",
      "    ) = None,\n",
      "    **kwargs: Unpack[DeprecatedKwargs]\n",
      ") -> Self\n",
      "\n",
      "```\n",
      "\n",
      "Add a new node to the state graph.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| node | str|StateNode[NodeInputT,ContextT] | The function or runnable this node will run.\n",
      "If a string is provided, it will be used as the node name, and action will be used as the function or runnable. | required |\n",
      "| action | StateNode[NodeInputT,ContextT] | None | The action associated with the node. (default: None)\n",
      "Will be used as the node function or runnable ifnodeis a string (node name). | None |\n",
      "| defer | bool | Whether to defer the execution of the node until the run is about to end. | False |\n",
      "| metadata | dict[str,Any] | None | The metadata associated with the node. (default: None) | None |\n",
      "| input_schema | type[NodeInputT] | None | The input schema for the node. (default: the graph's state schema) | None |\n",
      "| retry_policy | RetryPolicy|Sequence[RetryPolicy] | None | The retry policy for the node. (default: None)\n",
      "If a sequence is provided, the first matching policy will be applied. | None |\n",
      "| cache_policy | CachePolicy| None | The cache policy for the node. (default: None) | None |\n",
      "| destinations | dict[str,str] |tuple[str, ...] | None | Destinations that indicate where a node can route to.\n",
      "This is useful for edgeless graphs with nodes that returnCommandobjects.\n",
      "If a dict is provided, the keys will be used as the target node names and the values will be used as the labels for the edges.\n",
      "If a tuple is provided, the values will be used as the target node names.\n",
      "NOTE: this is only used for graph rendering and doesn't have any effect on the graph execution. | None |\n",
      "\n",
      "Example\n",
      "```python\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from langgraph.graph import START, StateGraph\n",
      "\n",
      "class State(TypedDict):\n",
      "    x: int\n",
      "\n",
      "def my_node(state: State, config: RunnableConfig) -> State:\n",
      "    return {\"x\": state[\"x\"] + 1}\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(my_node)  # node name will be 'my_node'\n",
      "builder.add_edge(START, \"my_node\")\n",
      "graph = builder.compile()\n",
      "graph.invoke({\"x\": 1})\n",
      "# {'x': 2}\n",
      "\n",
      "```\n",
      "\n",
      "Customize the name:\n",
      "```python\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"my_fair_node\", my_node)\n",
      "builder.add_edge(START, \"my_fair_node\")\n",
      "graph = builder.compile()\n",
      "graph.invoke({\"x\": 1})\n",
      "# {'x': 2}\n",
      "\n",
      "```\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| Self | Self | The instance of the state graph, allowing for method chaining. |\n",
      "\n",
      "### [add_edg](#langgraph.graph.state.StateGraph.add_edge)\n",
      "\n",
      "```python\n",
      "add_edge(start_key: str | list[str], end_key: str) -> Self\n",
      "\n",
      "```\n",
      "\n",
      "Add a directed edge from the start node (or list of start nodes) to the end node.\n",
      "\n",
      "When a single start node is provided, the graph will wait for that node to complete\n",
      "before executing the end node. When multiple start nodes are provided,\n",
      "the graph will wait for ALL of the start nodes to complete before executing the end node.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| start_key | str|list[str] | The key(s) of the start node(s) of the edge. | required |\n",
      "| end_key | str | The key of the end node of the edge. | required |\n",
      "\n",
      "Raises:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| ValueError | If the start key is 'END' or if the start key or end key is not present in the graph. |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| Self | Self | The instance of the state graph, allowing for method chaining. |\n",
      "\n",
      "### [add_conditional_edge](#langgraph.graph.state.StateGraph.add_conditional_edges)\n",
      "\n",
      "```python\n",
      "add_conditional_edges(\n",
      "    source: str,\n",
      "    path: (\n",
      "        Callable[..., Hashable | list[Hashable]]\n",
      "        | Callable[\n",
      "            ..., Awaitable[Hashable | list[Hashable]]\n",
      "        ]\n",
      "        | Runnable[Any, Hashable | list[Hashable]]\n",
      "    ),\n",
      "    path_map: dict[Hashable, str] | list[str] | None = None,\n",
      ") -> Self\n",
      "\n",
      "```\n",
      "\n",
      "Add a conditional edge from the starting node to any number of destination nodes.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| source | str | The starting node. This conditional edge will run when\n",
      "exiting this node. | required |\n",
      "| path | Callable[...,Hashable|list[Hashable]] |Callable[...,Awaitable[Hashable|list[Hashable]]] |Runnable[Any,Hashable|list[Hashable]] | The callable that determines the next\n",
      "node or nodes. If not specifyingpath_mapit should return one or\n",
      "more nodes. If it returns END, the graph will stop execution. | required |\n",
      "| path_map | dict[Hashable,str] |list[str] | None | Optional mapping of paths to node\n",
      "names. If omitted the paths returned bypathshould be node names. | None |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| Self | Self | The instance of the graph, allowing for method chaining. |\n",
      "\n",
      "Without typehints on the `path` function's return value (e.g., `-> Literal[\"foo\", \"__end__\"]:`)\n",
      "or a path_map, the graph visualization assumes the edge could transition to any node in the graph.\n",
      "\n",
      "### [add_sequenc](#langgraph.graph.state.StateGraph.add_sequence)\n",
      "\n",
      "```python\n",
      "add_sequence(\n",
      "    nodes: Sequence[\n",
      "        StateNode[NodeInputT, ContextT]\n",
      "        | tuple[str, StateNode[NodeInputT, ContextT]]\n",
      "    ],\n",
      ") -> Self\n",
      "\n",
      "```\n",
      "\n",
      "Add a sequence of nodes that will be executed in the provided order.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| nodes | Sequence[StateNode[NodeInputT,ContextT] |tuple[str,StateNode[NodeInputT,ContextT]]] | A sequence of StateNodes (callables that accept a state arg) or (name, StateNode) tuples.\n",
      "If no names are provided, the name will be inferred from the node object (e.g. a runnable or a callable name).\n",
      "Each node will be executed in the order provided. | required |\n",
      "\n",
      "Raises:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| ValueError | if the sequence is empty. |\n",
      "| ValueError | if the sequence contains duplicate node names. |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| Self | Self | The instance of the state graph, allowing for method chaining. |\n",
      "\n",
      "### [compil](#langgraph.graph.state.StateGraph.compile)\n",
      "\n",
      "```python\n",
      "compile(\n",
      "    checkpointer: Checkpointer = None,\n",
      "    *,\n",
      "    cache: BaseCache | None = None,\n",
      "    store: BaseStore | None = None,\n",
      "    interrupt_before: All | list[str] | None = None,\n",
      "    interrupt_after: All | list[str] | None = None,\n",
      "    debug: bool = False,\n",
      "    name: str | None = None\n",
      ") -> CompiledStateGraph[StateT, ContextT, InputT, OutputT]\n",
      "\n",
      "```\n",
      "\n",
      "Compiles the state graph into a `CompiledStateGraph` object.\n",
      "\n",
      "The compiled graph implements the `Runnable` interface and can be invoked,\n",
      "streamed, batched, and run asynchronously.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| checkpointer | Checkpointer | A checkpoint saver object or flag.\n",
      "If provided, this Checkpointer serves as a fully versioned \"short-term memory\" for the graph,\n",
      "allowing it to be paused, resumed, and replayed from any point.\n",
      "If None, it may inherit the parent graph's checkpointer when used as a subgraph.\n",
      "If False, it will not use or inherit any checkpointer. | None |\n",
      "| interrupt_before | All|list[str] | None | An optional list of node names to interrupt before. | None |\n",
      "| interrupt_after | All|list[str] | None | An optional list of node names to interrupt after. | None |\n",
      "| debug | bool | A flag indicating whether to enable debug mode. | False |\n",
      "| name | str| None | The name to use for the compiled graph. | None |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| CompiledStateGraph | CompiledStateGraph[StateT,ContextT,InputT,OutputT] | The compiled state graph. |\n",
      "\n",
      "## [CompiledStateGrap](#langgraph.graph.state.CompiledStateGraph)\n",
      "\n",
      "              Bases: `Pregel[StateT, ContextT, InputT, OutputT]`, `Generic[StateT, ContextT, InputT, OutputT]`\n",
      "\n",
      "Methods:\n",
      "\n",
      "| Name | Description |\n",
      "| ---- | ---- |\n",
      "| stream | Stream graph steps for a single input. |\n",
      "| astream | Asynchronously stream graph steps for a single input. |\n",
      "| invoke | Run the graph with a single input and config. |\n",
      "| ainvoke | Asynchronously invoke the graph on a single input. |\n",
      "| get_state | Get the current state of the graph. |\n",
      "| aget_state | Get the current state of the graph. |\n",
      "| get_state_history | Get the history of the state of the graph. |\n",
      "| aget_state_history | Asynchronously get the history of the state of the graph. |\n",
      "| update_state | Update the state of the graph with the given values, as if they came from |\n",
      "| aupdate_state | Asynchronously update the state of the graph with the given values, as if they came from |\n",
      "| bulk_update_state | Apply updates to the graph state in bulk. Requires a checkpointer to be set. |\n",
      "| abulk_update_state | Asynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set. |\n",
      "| get_graph | Return a drawable representation of the computation graph. |\n",
      "| aget_graph | Return a drawable representation of the computation graph. |\n",
      "| get_subgraphs | Get the subgraphs of the graph. |\n",
      "| aget_subgraphs | Get the subgraphs of the graph. |\n",
      "| with_config | Create a copy of the Pregel object with an updated config. |\n",
      "\n",
      "### [strea](#langgraph.graph.state.CompiledStateGraph.stream)\n",
      "\n",
      "```python\n",
      "stream(\n",
      "    input: InputT | Command | None,\n",
      "    config: RunnableConfig | None = None,\n",
      "    *,\n",
      "    context: ContextT | None = None,\n",
      "    stream_mode: (\n",
      "        StreamMode | Sequence[StreamMode] | None\n",
      "    ) = None,\n",
      "    print_mode: StreamMode | Sequence[StreamMode] = (),\n",
      "    output_keys: str | Sequence[str] | None = None,\n",
      "    interrupt_before: All | Sequence[str] | None = None,\n",
      "    interrupt_after: All | Sequence[str] | None = None,\n",
      "    durability: Durability | None = None,\n",
      "    subgraphs: bool = False,\n",
      "    debug: bool | None = None,\n",
      "    **kwargs: Unpack[DeprecatedKwargs]\n",
      ") -> Iterator[dict[str, Any] | Any]\n",
      "\n",
      "```\n",
      "\n",
      "Stream graph steps for a single input.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| input | InputT|Command| None | The input to the graph. | required |\n",
      "| config | RunnableConfig| None | The configuration to use for the run. | None |\n",
      "| context | ContextT| None | The static context to use for the run.Added in version 0.6.0. | None |\n",
      "| stream_mode | StreamMode|Sequence[StreamMode] | None | The mode to stream output, defaults toself.stream_mode.\n",
      "Options are:\"values\": Emit all values in the state after each step, including interrupts.\n",
      "    When used with functional API, values are emitted once at the end of the workflow.\"updates\": Emit only the node or task names and updates returned by the nodes or tasks after each step.\n",
      "    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\"custom\": Emit custom data from inside nodes or tasks usingStreamWriter.\"messages\": Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n",
      "    Will be emitted as 2-tuples(LLM token, metadata).\"checkpoints\": Emit an event when a checkpoint is created, in the same format as returned by get_state().\"tasks\": Emit events when tasks start and finish, including their results and errors.You can pass a list as thestream_modeparameter to stream multiple modes at once.\n",
      "The streamed outputs will be tuples of(mode, data).SeeLangGraph streaming guidefor more details. | None |\n",
      "| print_mode | StreamMode|Sequence[StreamMode] | Accepts the same values asstream_mode, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way. | () |\n",
      "| output_keys | str|Sequence[str] | None | The keys to stream, defaults to all non-context channels. | None |\n",
      "| interrupt_before | All|Sequence[str] | None | Nodes to interrupt before, defaults to all nodes in the graph. | None |\n",
      "| interrupt_after | All|Sequence[str] | None | Nodes to interrupt after, defaults to all nodes in the graph. | None |\n",
      "| durability | Durability| None | The durability mode for the graph execution, defaults to \"async\". Options are:\n",
      "-\"sync\": Changes are persisted synchronously before the next step starts.\n",
      "-\"async\": Changes are persisted asynchronously while the next step executes.\n",
      "-\"exit\": Changes are persisted only when the graph exits. | None |\n",
      "| subgraphs | bool | Whether to stream events from inside subgraphs, defaults to False.\n",
      "If True, the events will be emitted as tuples(namespace, data),\n",
      "or(namespace, mode, data)ifstream_modeis a list,\n",
      "wherenamespaceis a tuple with the path to the node where a subgraph is invoked,\n",
      "e.g.(\"parent_node:<task_id>\", \"child_node:<task_id>\").SeeLangGraph streaming guidefor more details. | False |\n",
      "\n",
      "Yields:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| dict[str,Any] |Any | The output of each step in the graph. The output shape depends on the stream_mode. |\n",
      "\n",
      "### [astreamasyn](#langgraph.graph.state.CompiledStateGraph.astream)\n",
      "\n",
      "```python\n",
      "astream(\n",
      "    input: InputT | Command | None,\n",
      "    config: RunnableConfig | None = None,\n",
      "    *,\n",
      "    context: ContextT | None = None,\n",
      "    stream_mode: (\n",
      "        StreamMode | Sequence[StreamMode] | None\n",
      "    ) = None,\n",
      "    print_mode: StreamMode | Sequence[StreamMode] = (),\n",
      "    output_keys: str | Sequence[str] | None = None,\n",
      "    interrupt_before: All | Sequence[str] | None = None,\n",
      "    interrupt_after: All | Sequence[str] | None = None,\n",
      "    durability: Durability | None = None,\n",
      "    subgraphs: bool = False,\n",
      "    debug: bool | None = None,\n",
      "    **kwargs: Unpack[DeprecatedKwargs]\n",
      ") -> AsyncIterator[dict[str, Any] | Any]\n",
      "\n",
      "```\n",
      "\n",
      "Asynchronously stream graph steps for a single input.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| input | InputT|Command| None | The input to the graph. | required |\n",
      "| config | RunnableConfig| None | The configuration to use for the run. | None |\n",
      "| context | ContextT| None | The static context to use for the run.Added in version 0.6.0. | None |\n",
      "| stream_mode | StreamMode|Sequence[StreamMode] | None | The mode to stream output, defaults toself.stream_mode.\n",
      "Options are:\"values\": Emit all values in the state after each step, including interrupts.\n",
      "    When used with functional API, values are emitted once at the end of the workflow.\"updates\": Emit only the node or task names and updates returned by the nodes or tasks after each step.\n",
      "    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\"custom\": Emit custom data from inside nodes or tasks usingStreamWriter.\"messages\": Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n",
      "    Will be emitted as 2-tuples(LLM token, metadata).\"debug\": Emit debug events with as much information as possible for each step.You can pass a list as thestream_modeparameter to stream multiple modes at once.\n",
      "The streamed outputs will be tuples of(mode, data).SeeLangGraph streaming guidefor more details. | None |\n",
      "| print_mode | StreamMode|Sequence[StreamMode] | Accepts the same values asstream_mode, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way. | () |\n",
      "| output_keys | str|Sequence[str] | None | The keys to stream, defaults to all non-context channels. | None |\n",
      "| interrupt_before | All|Sequence[str] | None | Nodes to interrupt before, defaults to all nodes in the graph. | None |\n",
      "| interrupt_after | All|Sequence[str] | None | Nodes to interrupt after, defaults to all nodes in the graph. | None |\n",
      "| durability | Durability| None | The durability mode for the graph execution, defaults to \"async\". Options are:\n",
      "-\"sync\": Changes are persisted synchronously before the next step starts.\n",
      "-\"async\": Changes are persisted asynchronously while the next step executes.\n",
      "-\"exit\": Changes are persisted only when the graph exits. | None |\n",
      "| subgraphs | bool | Whether to stream events from inside subgraphs, defaults to False.\n",
      "If True, the events will be emitted as tuples(namespace, data),\n",
      "or(namespace, mode, data)ifstream_modeis a list,\n",
      "wherenamespaceis a tuple with the path to the node where a subgraph is invoked,\n",
      "e.g.(\"parent_node:<task_id>\", \"child_node:<task_id>\").SeeLangGraph streaming guidefor more details. | False |\n",
      "\n",
      "Yields:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| AsyncIterator[dict[str,Any] |Any] | The output of each step in the graph. The output shape depends on the stream_mode. |\n",
      "\n",
      "### [invok](#langgraph.graph.state.CompiledStateGraph.invoke)\n",
      "\n",
      "```python\n",
      "invoke(\n",
      "    input: InputT | Command | None,\n",
      "    config: RunnableConfig | None = None,\n",
      "    *,\n",
      "    context: ContextT | None = None,\n",
      "    stream_mode: StreamMode = \"values\",\n",
      "    print_mode: StreamMode | Sequence[StreamMode] = (),\n",
      "    output_keys: str | Sequence[str] | None = None,\n",
      "    interrupt_before: All | Sequence[str] | None = None,\n",
      "    interrupt_after: All | Sequence[str] | None = None,\n",
      "    durability: Durability | None = None,\n",
      "    **kwargs: Any\n",
      ") -> dict[str, Any] | Any\n",
      "\n",
      "```\n",
      "\n",
      "Run the graph with a single input and config.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| input | InputT|Command| None | The input data for the graph. It can be a dictionary or any other type. | required |\n",
      "| config | RunnableConfig| None | Optional. The configuration for the graph run. | None |\n",
      "| context | ContextT| None | The static context to use for the run.Added in version 0.6.0. | None |\n",
      "| stream_mode | StreamMode | Optional[str]. The stream mode for the graph run. Default is \"values\". | 'values' |\n",
      "| print_mode | StreamMode|Sequence[StreamMode] | Accepts the same values asstream_mode, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way. | () |\n",
      "| output_keys | str|Sequence[str] | None | Optional. The output keys to retrieve from the graph run. | None |\n",
      "| interrupt_before | All|Sequence[str] | None | Optional. The nodes to interrupt the graph run before. | None |\n",
      "| interrupt_after | All|Sequence[str] | None | Optional. The nodes to interrupt the graph run after. | None |\n",
      "| durability | Durability| None | The durability mode for the graph execution, defaults to \"async\". Options are:\n",
      "-\"sync\": Changes are persisted synchronously before the next step starts.\n",
      "-\"async\": Changes are persisted asynchronously while the next step executes.\n",
      "-\"exit\": Changes are persisted only when the graph exits. | None |\n",
      "| **kwargs | Any | Additional keyword arguments to pass to the graph run. | {} |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| dict[str,Any] |Any | The output of the graph run. If stream_mode is \"values\", it returns the latest output. |\n",
      "| dict[str,Any] |Any | If stream_mode is not \"values\", it returns a list of output chunks. |\n",
      "\n",
      "### [ainvokeasyn](#langgraph.graph.state.CompiledStateGraph.ainvoke)\n",
      "\n",
      "```python\n",
      "ainvoke(\n",
      "    input: InputT | Command | None,\n",
      "    config: RunnableConfig | None = None,\n",
      "    *,\n",
      "    context: ContextT | None = None,\n",
      "    stream_mode: StreamMode = \"values\",\n",
      "    print_mode: StreamMode | Sequence[StreamMode] = (),\n",
      "    output_keys: str | Sequence[str] | None = None,\n",
      "    interrupt_before: All | Sequence[str] | None = None,\n",
      "    interrupt_after: All | Sequence[str] | None = None,\n",
      "    durability: Durability | None = None,\n",
      "    **kwargs: Any\n",
      ") -> dict[str, Any] | Any\n",
      "\n",
      "```\n",
      "\n",
      "Asynchronously invoke the graph on a single input.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| input | InputT|Command| None | The input data for the computation. It can be a dictionary or any other type. | required |\n",
      "| config | RunnableConfig| None | Optional. The configuration for the computation. | None |\n",
      "| context | ContextT| None | The static context to use for the run.Added in version 0.6.0. | None |\n",
      "| stream_mode | StreamMode | Optional. The stream mode for the computation. Default is \"values\". | 'values' |\n",
      "| print_mode | StreamMode|Sequence[StreamMode] | Accepts the same values asstream_mode, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way. | () |\n",
      "| output_keys | str|Sequence[str] | None | Optional. The output keys to include in the result. Default is None. | None |\n",
      "| interrupt_before | All|Sequence[str] | None | Optional. The nodes to interrupt before. Default is None. | None |\n",
      "| interrupt_after | All|Sequence[str] | None | Optional. The nodes to interrupt after. Default is None. | None |\n",
      "| durability | Durability| None | The durability mode for the graph execution, defaults to \"async\". Options are:\n",
      "-\"sync\": Changes are persisted synchronously before the next step starts.\n",
      "-\"async\": Changes are persisted asynchronously while the next step executes.\n",
      "-\"exit\": Changes are persisted only when the graph exits. | None |\n",
      "| **kwargs | Any | Additional keyword arguments. | {} |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| dict[str,Any] |Any | The result of the computation. If stream_mode is \"values\", it returns the latest value. |\n",
      "| dict[str,Any] |Any | If stream_mode is \"chunks\", it returns a list of chunks. |\n",
      "\n",
      "### [get_stat](#langgraph.graph.state.CompiledStateGraph.get_state)\n",
      "\n",
      "```python\n",
      "get_state(\n",
      "    config: RunnableConfig, *, subgraphs: bool = False\n",
      ") -> StateSnapshot\n",
      "\n",
      "```\n",
      "\n",
      "Get the current state of the graph.\n",
      "\n",
      "### [aget_stateasyn](#langgraph.graph.state.CompiledStateGraph.aget_state)\n",
      "\n",
      "```python\n",
      "aget_state(\n",
      "    config: RunnableConfig, *, subgraphs: bool = False\n",
      ") -> StateSnapshot\n",
      "\n",
      "```\n",
      "\n",
      "Get the current state of the graph.\n",
      "\n",
      "### [get_state_histor](#langgraph.graph.state.CompiledStateGraph.get_state_history)\n",
      "\n",
      "```python\n",
      "get_state_history(\n",
      "    config: RunnableConfig,\n",
      "    *,\n",
      "    filter: dict[str, Any] | None = None,\n",
      "    before: RunnableConfig | None = None,\n",
      "    limit: int | None = None\n",
      ") -> Iterator[StateSnapshot]\n",
      "\n",
      "```\n",
      "\n",
      "Get the history of the state of the graph.\n",
      "\n",
      "### [aget_state_historyasyn](#langgraph.graph.state.CompiledStateGraph.aget_state_history)\n",
      "\n",
      "```python\n",
      "aget_state_history(\n",
      "    config: RunnableConfig,\n",
      "    *,\n",
      "    filter: dict[str, Any] | None = None,\n",
      "    before: RunnableConfig | None = None,\n",
      "    limit: int | None = None\n",
      ") -> AsyncIterator[StateSnapshot]\n",
      "\n",
      "```\n",
      "\n",
      "Asynchronously get the history of the state of the graph.\n",
      "\n",
      "### [update_stat](#langgraph.graph.state.CompiledStateGraph.update_state)\n",
      "\n",
      "```python\n",
      "update_state(\n",
      "    config: RunnableConfig,\n",
      "    values: dict[str, Any] | Any | None,\n",
      "    as_node: str | None = None,\n",
      "    task_id: str | None = None,\n",
      ") -> RunnableConfig\n",
      "\n",
      "```\n",
      "\n",
      "Update the state of the graph with the given values, as if they came from\n",
      "node `as_node`. If `as_node` is not provided, it will be set to the last node\n",
      "that updated the state, if not ambiguous.\n",
      "\n",
      "### [aupdate_stateasyn](#langgraph.graph.state.CompiledStateGraph.aupdate_state)\n",
      "\n",
      "```python\n",
      "aupdate_state(\n",
      "    config: RunnableConfig,\n",
      "    values: dict[str, Any] | Any,\n",
      "    as_node: str | None = None,\n",
      "    task_id: str | None = None,\n",
      ") -> RunnableConfig\n",
      "\n",
      "```\n",
      "\n",
      "Asynchronously update the state of the graph with the given values, as if they came from\n",
      "node `as_node`. If `as_node` is not provided, it will be set to the last node\n",
      "that updated the state, if not ambiguous.\n",
      "\n",
      "### [bulk_update_stat](#langgraph.graph.state.CompiledStateGraph.bulk_update_state)\n",
      "\n",
      "```python\n",
      "bulk_update_state(\n",
      "    config: RunnableConfig,\n",
      "    supersteps: Sequence[Sequence[StateUpdate]],\n",
      ") -> RunnableConfig\n",
      "\n",
      "```\n",
      "\n",
      "Apply updates to the graph state in bulk. Requires a checkpointer to be set.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| config | RunnableConfig | The config to apply the updates to. | required |\n",
      "| supersteps | Sequence[Sequence[StateUpdate]] | A list of supersteps, each including a list of updates to apply sequentially to a graph state.\n",
      "        Each update is a tuple of the form(values, as_node, task_id)where task_id is optional. | required |\n",
      "\n",
      "Raises:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| ValueError | If no checkpointer is set or no updates are provided. |\n",
      "| InvalidUpdateError | If an invalid update is provided. |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| RunnableConfig | RunnableConfig | The updated config. |\n",
      "\n",
      "### [abulk_update_stateasyn](#langgraph.graph.state.CompiledStateGraph.abulk_update_state)\n",
      "\n",
      "```python\n",
      "abulk_update_state(\n",
      "    config: RunnableConfig,\n",
      "    supersteps: Sequence[Sequence[StateUpdate]],\n",
      ") -> RunnableConfig\n",
      "\n",
      "```\n",
      "\n",
      "Asynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| config | RunnableConfig | The config to apply the updates to. | required |\n",
      "| supersteps | Sequence[Sequence[StateUpdate]] | A list of supersteps, each including a list of updates to apply sequentially to a graph state.\n",
      "        Each update is a tuple of the form(values, as_node, task_id)where task_id is optional. | required |\n",
      "\n",
      "Raises:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| ValueError | If no checkpointer is set or no updates are provided. |\n",
      "| InvalidUpdateError | If an invalid update is provided. |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Name | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| RunnableConfig | RunnableConfig | The updated config. |\n",
      "\n",
      "### [get_grap](#langgraph.graph.state.CompiledStateGraph.get_graph)\n",
      "\n",
      "```python\n",
      "get_graph(\n",
      "    config: RunnableConfig | None = None,\n",
      "    *,\n",
      "    xray: int | bool = False\n",
      ") -> Graph\n",
      "\n",
      "```\n",
      "\n",
      "Return a drawable representation of the computation graph.\n",
      "\n",
      "### [aget_graphasyn](#langgraph.graph.state.CompiledStateGraph.aget_graph)\n",
      "\n",
      "```python\n",
      "aget_graph(\n",
      "    config: RunnableConfig | None = None,\n",
      "    *,\n",
      "    xray: int | bool = False\n",
      ") -> Graph\n",
      "\n",
      "```\n",
      "\n",
      "Return a drawable representation of the computation graph.\n",
      "\n",
      "### [get_subgraph](#langgraph.graph.state.CompiledStateGraph.get_subgraphs)\n",
      "\n",
      "```python\n",
      "get_subgraphs(\n",
      "    *, namespace: str | None = None, recurse: bool = False\n",
      ") -> Iterator[tuple[str, PregelProtocol]]\n",
      "\n",
      "```\n",
      "\n",
      "Get the subgraphs of the graph.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| namespace | str| None | The namespace to filter the subgraphs by. | None |\n",
      "| recurse | bool | Whether to recurse into the subgraphs.\n",
      "If False, only the immediate subgraphs will be returned. | False |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| Iterator[tuple[str,PregelProtocol]] | Iterator[tuple[str, PregelProtocol]]: An iterator of the (namespace, subgraph) pairs. |\n",
      "\n",
      "### [aget_subgraphsasyn](#langgraph.graph.state.CompiledStateGraph.aget_subgraphs)\n",
      "\n",
      "```python\n",
      "aget_subgraphs(\n",
      "    *, namespace: str | None = None, recurse: bool = False\n",
      ") -> AsyncIterator[tuple[str, PregelProtocol]]\n",
      "\n",
      "```\n",
      "\n",
      "Get the subgraphs of the graph.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| namespace | str| None | The namespace to filter the subgraphs by. | None |\n",
      "| recurse | bool | Whether to recurse into the subgraphs.\n",
      "If False, only the immediate subgraphs will be returned. | False |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| AsyncIterator[tuple[str,PregelProtocol]] | AsyncIterator[tuple[str, PregelProtocol]]: An iterator of the (namespace, subgraph) pairs. |\n",
      "\n",
      "### [with_confi](#langgraph.graph.state.CompiledStateGraph.with_config)\n",
      "\n",
      "```python\n",
      "with_config(\n",
      "    config: RunnableConfig | None = None, **kwargs: Any\n",
      ") -> Self\n",
      "\n",
      "```\n",
      "\n",
      "Create a copy of the Pregel object with an updated config.\n",
      "\n",
      "Functions:\n",
      "\n",
      "| Name | Description |\n",
      "| ---- | ---- |\n",
      "| add_messages | Merges two lists of messages, updating existing messages by ID. |\n",
      "\n",
      "## [add_message](#langgraph.graph.message.add_messages)\n",
      "\n",
      "```python\n",
      "add_messages(\n",
      "    left: Messages,\n",
      "    right: Messages,\n",
      "    *,\n",
      "    format: Literal[\"langchain-openai\"] | None = None\n",
      ") -> Messages\n",
      "\n",
      "```\n",
      "\n",
      "Merges two lists of messages, updating existing messages by ID.\n",
      "\n",
      "By default, this ensures the state is \"append-only\", unless the\n",
      "new message has the same ID as an existing message.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "| Name | Type | Description | Default |\n",
      "| ---- | ---- | ---- | ---- |\n",
      "| left | Messages | The base list of messages. | required |\n",
      "| right | Messages | The list of messages (or single message) to merge\n",
      "into the base list. | required |\n",
      "| format | Literal['langchain-openai'] | None | The format to return messages in. If None then messages will be\n",
      "returned as is. If 'langchain-openai' then messages will be returned as\n",
      "BaseMessage objects with their contents formatted to match OpenAI message\n",
      "format, meaning contents can be string, 'text' blocks, or 'image_url' blocks\n",
      "and tool responses are returned as their own ToolMessages.RequirementMust havelangchain-core>=0.3.11installed to use this feature. | None |\n",
      "\n",
      "Returns:\n",
      "\n",
      "| Type | Description |\n",
      "| ---- | ---- |\n",
      "| Messages | A new list of messages with the messages fromrightmerged intoleft. |\n",
      "| Messages | If a message inrighthas the same ID as a message inleft, the |\n",
      "| Messages | message fromrightwill replace the message fromleft. |\n",
      "\n",
      "Example\n",
      "Basic usage```python\n",
      "from langchain_core.messages import AIMessage, HumanMessage\n",
      "msgs1 = [HumanMessage(content=\"Hello\", id=\"1\")]\n",
      "msgs2 = [AIMessage(content=\"Hi there!\", id=\"2\")]\n",
      "add_messages(msgs1, msgs2)\n",
      "# [HumanMessage(content='Hello', id='1'), AIMessage(content='Hi there!', id='2')]\n",
      "\n",
      "```\n",
      "\n",
      "Overwrite existing message```python\n",
      "msgs1 = [HumanMessage(content=\"Hello\", id=\"1\")]\n",
      "msgs2 = [HumanMessage(content=\"Hello again\", id=\"1\")]\n",
      "add_messages(msgs1, msgs2)\n",
      "# [HumanMessage(content='Hello again', id='1')]\n",
      "\n",
      "```\n",
      "\n",
      "Use in a StateGraph```python\n",
      "from typing import Annotated\n",
      "from typing_extensions import TypedDict\n",
      "from langgraph.graph import StateGraph\n",
      "\n",
      "class State(TypedDict):\n",
      "    messages: Annotated[list, add_messages]\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"chatbot\", lambda state: {\"messages\": [(\"assistant\", \"Hello\")]})\n",
      "builder.set_entry_point(\"chatbot\")\n",
      "builder.set_finish_point(\"chatbot\")\n",
      "graph = builder.compile()\n",
      "graph.invoke({})\n",
      "# {'messages': [AIMessage(content='Hello', id=...)]}\n",
      "\n",
      "```\n",
      "\n",
      "Use OpenAI message format```python\n",
      "from typing import Annotated\n",
      "from typing_extensions import TypedDict\n",
      "from langgraph.graph import StateGraph, add_messages\n",
      "\n",
      "class State(TypedDict):\n",
      "    messages: Annotated[list, add_messages(format='langchain-openai')]\n",
      "\n",
      "def chatbot_node(state: State) -> list:\n",
      "    return {\"messages\": [\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"type\": \"text\",\n",
      "                    \"text\": \"Here's an image:\",\n",
      "                    \"cache_control\": {\"type\": \"ephemeral\"},\n",
      "                },\n",
      "                {\n",
      "                    \"type\": \"image\",\n",
      "                    \"source\": {\n",
      "                        \"type\": \"base64\",\n",
      "                        \"media_type\": \"image/jpeg\",\n",
      "                        \"data\": \"1234\",\n",
      "                    },\n",
      "                },\n",
      "            ]\n",
      "        },\n",
      "    ]}\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"chatbot\", chatbot_node)\n",
      "builder.set_entry_point(\"chatbot\")\n",
      "builder.set_finish_point(\"chatbot\")\n",
      "graph = builder.compile()\n",
      "graph.invoke({\"messages\": []})\n",
      "# {\n",
      "#     'messages': [\n",
      "#         HumanMessage(\n",
      "#             content=[\n",
      "#                 {\"type\": \"text\", \"text\": \"Here's an image:\"},\n",
      "#                 {\n",
      "#                     \"type\": \"image_url\",\n",
      "#                     \"image_url\": {\"url\": \"data:image/jpeg;base64,1234\"},\n",
      "#                 },\n",
      "#             ],\n",
      "#         ),\n",
      "#     ]\n",
      "# }\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 获取当前 notebook 的工作目录（通常是 .ipynb 所在目录）\n",
    "notebook_dir = os.getcwd()\n",
    "target_path = os.path.abspath(os.path.join(notebook_dir, \"../\"))\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "from src.ingest.parsers.langgraph_recursive_url import (\n",
    "    langgraph_recursive_url_extractor,\n",
    "    langgraph_recursive_url_metadata_extractor,\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://langchain-ai.github.io/langgraph/reference/graphs/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "metadata = langgraph_recursive_url_metadata_extractor(\n",
    "    raw_html=response.text,\n",
    "    url=url,\n",
    "    response=response,\n",
    "    type=\"api\",\n",
    "    lang=\"python\",\n",
    ")\n",
    "print(metadata)\n",
    "doc = langgraph_recursive_url_extractor(soup)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019335a0",
   "metadata": {},
   "source": [
    "### LangChain API Loader Test\n",
    "\n",
    "exclude dir https://python.langchain.com/api_reference/_modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af54cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'source': 'https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html#langchain_deepseek.chat_models.ChatDeepSeek', 'title': 'ChatDeepSeek — 🦜🔗 LangChain  documentation', 'type': 'api', 'lang': 'python'}\n",
      "# ChatDeepSeek#\n",
      "\n",
      "class langchain_deepseek.chat_models.ChatDeepSeek[source]#\n",
      "\n",
      "Bases: [BaseChatOpenAI](../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_openai.chat_models.base.BaseChatOpenAI)\n",
      "\n",
      "DeepSeek chat model integration to access models hosted in DeepSeek’s API.\n",
      "\n",
      "Setup:\n",
      "Install langchain-deepseek and set environment variable DEEPSEEK_API_KEY.\n",
      "\n",
      "```bash\n",
      "pip install -U langchain-deepseek\n",
      "export DEEPSEEK_API_KEY=\"your-api-key\"\n",
      "\n",
      "```\n",
      "\n",
      "Key init args — completion params:\n",
      "\n",
      "model: str\n",
      "Name of DeepSeek model to use, e.g. “deepseek-chat”.\n",
      "\n",
      "temperature: float\n",
      "Sampling temperature.\n",
      "\n",
      "max_tokens: Optional[int]\n",
      "Max number of tokens to generate.\n",
      "\n",
      "Key init args — client params:\n",
      "\n",
      "timeout: Optional[float]\n",
      "Timeout for requests.\n",
      "\n",
      "max_retries: int\n",
      "Max number of retries.\n",
      "\n",
      "api_key: Optional[str]\n",
      "DeepSeek API key. If not passed in will be read from env var DEEPSEEK_API_KEY.\n",
      "\n",
      "See full list of supported init args and their descriptions in the params section.\n",
      "\n",
      "Instantiate:\n",
      "```python\n",
      "from langchain_deepseek import ChatDeepSeek\n",
      "\n",
      "llm = ChatDeepSeek(\n",
      "    model=\"...\",\n",
      "    temperature=0,\n",
      "    max_tokens=None,\n",
      "    timeout=None,\n",
      "    max_retries=2,\n",
      "    # api_key=\"...\",\n",
      "    # other params...\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "Invoke:\n",
      "```python\n",
      "messages = [\n",
      "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
      "    (\"human\", \"I love programming.\"),\n",
      "]\n",
      "llm.invoke(messages)\n",
      "\n",
      "```\n",
      "\n",
      "Stream:\n",
      "```python\n",
      "for chunk in llm.stream(messages):\n",
      "    print(chunk.text(), end=\"\")\n",
      "\n",
      "```\n",
      "\n",
      "```python\n",
      "stream = llm.stream(messages)\n",
      "full = next(stream)\n",
      "for chunk in stream:\n",
      "    full += chunk\n",
      "full\n",
      "\n",
      "```\n",
      "\n",
      "Async:\n",
      "```python\n",
      "await llm.ainvoke(messages)\n",
      "\n",
      "# stream:\n",
      "# async for chunk in (await llm.astream(messages))\n",
      "\n",
      "# batch:\n",
      "# await llm.abatch([messages])\n",
      "\n",
      "```\n",
      "\n",
      "Tool calling:\n",
      "```python\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class GetWeather(BaseModel):\n",
      "    '''Get the current weather in a given location'''\n",
      "\n",
      "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      "\n",
      "class GetPopulation(BaseModel):\n",
      "    '''Get the current population in a given location'''\n",
      "\n",
      "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      "\n",
      "llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
      "ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n",
      "ai_msg.tool_calls\n",
      "\n",
      "```\n",
      "\n",
      "See ChatDeepSeek.bind_tools() method for more.\n",
      "\n",
      "Structured output:\n",
      "```python\n",
      "from typing import Optional\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class Joke(BaseModel):\n",
      "    '''Joke to tell user.'''\n",
      "\n",
      "    setup: str = Field(description=\"The setup of the joke\")\n",
      "    punchline: str = Field(description=\"The punchline to the joke\")\n",
      "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
      "\n",
      "structured_llm = llm.with_structured_output(Joke)\n",
      "structured_llm.invoke(\"Tell me a joke about cats\")\n",
      "\n",
      "```\n",
      "\n",
      "See ChatDeepSeek.with_structured_output() for more.\n",
      "\n",
      "Token usage:\n",
      "```python\n",
      "ai_msg = llm.invoke(messages)\n",
      "ai_msg.usage_metadata\n",
      "\n",
      "```\n",
      "\n",
      "```python\n",
      "{'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33}\n",
      "\n",
      "```\n",
      "\n",
      "Response metadata\n",
      "```python\n",
      "ai_msg = llm.invoke(messages)\n",
      "ai_msg.response_metadata\n",
      "\n",
      "```\n",
      "\n",
      "Note\n",
      "\n",
      "ChatDeepSeek implements the standard [Runnable Interface](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable). 🏃\n",
      "\n",
      "The [Runnable Interface](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) has additional methods that are available on runnables, such as [with_config](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config), [with_types](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_types), [with_retry](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_retry), [assign](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.assign), [bind](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.bind), [get_graph](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.get_graph), and more.\n",
      "\n",
      "param api_base: str [Optional]#\n",
      "\n",
      "DeepSeek API base URL\n",
      "\n",
      "param api_key: SecretStr | None [Optional]#\n",
      "\n",
      "DeepSeek API key\n",
      "\n",
      "param cache: BaseCache | bool | None = None#\n",
      "\n",
      "Whether to cache the response.\n",
      "\n",
      "- If true, will use the global cache.\n",
      "\n",
      "- If false, will not use a cache\n",
      "\n",
      "- If None, will use the global cache if it’s set, otherwise no cache.\n",
      "\n",
      "- If instance of BaseCache, will use the provided cache.\n",
      "\n",
      "Caching is not currently supported for streaming methods of models.\n",
      "\n",
      "param callback_manager: BaseCallbackManager | None = None#\n",
      "\n",
      "Deprecated since version 0.1.7: Use [callbacks()](#langchain_deepseek.chat_models.ChatDeepSeek.callbacks) instead. It will be removed in pydantic==1.0.\n",
      "\n",
      "Callback manager to add to the run trace.\n",
      "\n",
      "param callbacks: Callbacks = None#\n",
      "\n",
      "Callbacks to add to the run trace.\n",
      "\n",
      "param custom_get_token_ids: Callable[[str], list[int]] | None = None#\n",
      "\n",
      "Optional encoder to use for counting tokens.\n",
      "\n",
      "param default_headers: Mapping[str, str] | None = None#\n",
      "\n",
      "param default_query: Mapping[str, object] | None = None#\n",
      "\n",
      "param disable_streaming: bool | Literal['tool_calling'] = False#\n",
      "\n",
      "Whether to disable streaming for this model.\n",
      "\n",
      "If streaming is bypassed, then stream()/astream()/astream_events() will\n",
      "defer to invoke()/ainvoke().\n",
      "\n",
      "- If True, will always bypass streaming case.\n",
      "\n",
      "- If 'tool_calling', will bypass streaming case only when the model is called\n",
      "with a tools keyword argument. In other words, LangChain will automatically\n",
      "switch to non-streaming behavior (invoke()) only when the tools argument is\n",
      "provided. This offers the best of both worlds.\n",
      "\n",
      "- If False (default), will always use streaming case if available.\n",
      "\n",
      "The main reason for this flag is that code might be written using .stream() and\n",
      "a user may want to swap out a given model for another model whose the implementation\n",
      "does not properly support streaming.\n",
      "\n",
      "param disabled_params: dict[str, Any] | None = None#\n",
      "\n",
      "Parameters of the OpenAI client or chat.completions endpoint that should be\n",
      "disabled for the given model.\n",
      "\n",
      "Should be specified as {\"param\": None | ['val1', 'val2']} where the key is the\n",
      "parameter and the value is either None, meaning that parameter should never be\n",
      "used, or it’s a list of disabled values for the parameter.\n",
      "\n",
      "For example, older models may not support the 'parallel_tool_calls' parameter at\n",
      "all, in which case disabled_params={\"parallel_tool_calls\": None} can be passed\n",
      "in.\n",
      "\n",
      "If a parameter is disabled then it will not be used by default in any methods, e.g.\n",
      "in [with_structured_output()](../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.with_structured_output).\n",
      "However this does not prevent a user from directly passed in the parameter during\n",
      "invocation.\n",
      "\n",
      "param extra_body: Mapping[str, Any] | None = None#\n",
      "\n",
      "Optional additional JSON properties to include in the request parameters when\n",
      "making requests to OpenAI compatible APIs, such as vLLM, LM Studio, or other\n",
      "providers.\n",
      "\n",
      "This is the recommended way to pass custom parameters that are specific to your\n",
      "OpenAI-compatible API provider but not part of the standard OpenAI API.\n",
      "\n",
      "Examples\n",
      "\n",
      "- LM Studio TTL parameter: extra_body={\"ttl\": 300}\n",
      "\n",
      "- vLLM custom parameters: extra_body={\"use_beam_search\": True}\n",
      "\n",
      "- Any other provider-specific parameters\n",
      "\n",
      "Note\n",
      "\n",
      "Do NOT use model_kwargs for custom parameters that are not part of the\n",
      "standard OpenAI API, as this will cause errors when making API calls. Use\n",
      "extra_body instead.\n",
      "\n",
      "param frequency_penalty: float | None = None#\n",
      "\n",
      "Penalizes repeated tokens according to frequency.\n",
      "\n",
      "param http_async_client: Any | None = None#\n",
      "\n",
      "Optional httpx.AsyncClient. Only used for async invocations. Must specify\n",
      "http_client as well if you’d like a custom client for sync invocations.\n",
      "\n",
      "param http_client: Any | None = None#\n",
      "\n",
      "Optional httpx.Client. Only used for sync invocations. Must specify\n",
      "http_async_client as well if you’d like a custom client for async\n",
      "invocations.\n",
      "\n",
      "param include: list[str] | None = None#\n",
      "\n",
      "Additional fields to include in generations from Responses API.\n",
      "\n",
      "Supported values:\n",
      "\n",
      "- 'file_search_call.results'\n",
      "\n",
      "- 'message.input_image.image_url'\n",
      "\n",
      "- 'computer_call_output.output.image_url'\n",
      "\n",
      "- 'reasoning.encrypted_content'\n",
      "\n",
      "- 'code_interpreter_call.outputs'\n",
      "\n",
      "Added in version 0.3.24.\n",
      "\n",
      "param include_response_headers: bool = False#\n",
      "\n",
      "Whether to include response headers in the output message response_metadata.\n",
      "\n",
      "param logit_bias: dict[int, int] | None = None#\n",
      "\n",
      "Modify the likelihood of specified tokens appearing in the completion.\n",
      "\n",
      "param logprobs: bool | None = None#\n",
      "\n",
      "Whether to return logprobs.\n",
      "\n",
      "param max_retries: int | None = None#\n",
      "\n",
      "Maximum number of retries to make when generating.\n",
      "\n",
      "param max_tokens: int | None = None#\n",
      "\n",
      "Maximum number of tokens to generate.\n",
      "\n",
      "param metadata: dict[str, Any] | None = None#\n",
      "\n",
      "Metadata to add to the run trace.\n",
      "\n",
      "param model_kwargs: dict[str, Any] [Optional]#\n",
      "\n",
      "Holds any model parameters valid for create call not explicitly specified.\n",
      "\n",
      "param model_name: str [Required] (alias 'model')#\n",
      "\n",
      "The name of the model\n",
      "\n",
      "param n: int | None = None#\n",
      "\n",
      "Number of chat completions to generate for each prompt.\n",
      "\n",
      "param openai_api_base: str | None = None (alias 'base_url')#\n",
      "\n",
      "Base URL path for API requests, leave blank if not using a proxy or service\n",
      "emulator.\n",
      "\n",
      "param openai_api_key: SecretStr | None [Optional] (alias 'api_key')#\n",
      "\n",
      "param openai_organization: str | None = None (alias 'organization')#\n",
      "\n",
      "Automatically inferred from env var OPENAI_ORG_ID if not provided.\n",
      "\n",
      "param openai_proxy: str | None [Optional]#\n",
      "\n",
      "param output_version: Literal['v0', 'responses/v1'] = 'v0'#\n",
      "\n",
      "Version of AIMessage output format to use.\n",
      "\n",
      "This field is used to roll-out new output formats for chat model AIMessages\n",
      "in a backwards-compatible way.\n",
      "\n",
      "Supported values:\n",
      "\n",
      "- 'v0': AIMessage format as of langchain-openai 0.3.x.\n",
      "\n",
      "- 'responses/v1': Formats Responses API output\n",
      "items into AIMessage content blocks.\n",
      "\n",
      "Currently only impacts the Responses API. output_version='responses/v1' is\n",
      "recommended.\n",
      "\n",
      "Added in version 0.3.25.\n",
      "\n",
      "param presence_penalty: float | None = None#\n",
      "\n",
      "Penalizes repeated tokens.\n",
      "\n",
      "param rate_limiter: BaseRateLimiter | None = None#\n",
      "\n",
      "An optional rate limiter to use for limiting the number of requests.\n",
      "\n",
      "param reasoning: dict[str, Any] | None = None#\n",
      "\n",
      "Reasoning parameters for reasoning models, i.e., OpenAI o-series models (o1, o3,\n",
      "o4-mini, etc.). For use with the Responses API.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "reasoning={\n",
      "    \"effort\": \"medium\",  # can be \"low\", \"medium\", or \"high\"\n",
      "    \"summary\": \"auto\",  # can be \"auto\", \"concise\", or \"detailed\"\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "Added in version 0.3.24.\n",
      "\n",
      "param reasoning_effort: str | None = None#\n",
      "\n",
      "Constrains effort on reasoning for reasoning models. For use with the Chat\n",
      "Completions API.\n",
      "\n",
      "Reasoning models only, like OpenAI o1, o3, and o4-mini.\n",
      "\n",
      "Currently supported values are low, medium, and high. Reducing reasoning effort\n",
      "can result in faster responses and fewer tokens used on reasoning in a response.\n",
      "\n",
      "Added in version 0.2.14.\n",
      "\n",
      "param request_timeout: float | tuple[float, float] | Any | None = None (alias 'timeout')#\n",
      "\n",
      "Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or\n",
      "None.\n",
      "\n",
      "param seed: int | None = None#\n",
      "\n",
      "Seed for generation\n",
      "\n",
      "param service_tier: str | None = None#\n",
      "\n",
      "Latency tier for request. Options are 'auto', 'default', or 'flex'.\n",
      "Relevant for users of OpenAI’s scale tier service.\n",
      "\n",
      "param stop: list[str] | str | None = None (alias 'stop_sequences')#\n",
      "\n",
      "Default stop sequences.\n",
      "\n",
      "param store: bool | None = None#\n",
      "\n",
      "If True, OpenAI may store response data for future use. Defaults to True\n",
      "for the Responses API and False for the Chat Completions API.\n",
      "\n",
      "Added in version 0.3.24.\n",
      "\n",
      "param stream_usage: bool = False#\n",
      "\n",
      "Whether to include usage metadata in streaming output. If True, an additional\n",
      "message chunk will be generated during the stream including usage metadata.\n",
      "\n",
      "Added in version 0.3.9.\n",
      "\n",
      "param streaming: bool = False#\n",
      "\n",
      "Whether to stream the results or not.\n",
      "\n",
      "param tags: list[str] | None = None#\n",
      "\n",
      "Tags to add to the run trace.\n",
      "\n",
      "param temperature: float | None = None#\n",
      "\n",
      "What sampling temperature to use.\n",
      "\n",
      "param tiktoken_model_name: str | None = None#\n",
      "\n",
      "The model name to pass to tiktoken when using this class.\n",
      "Tiktoken is used to count the number of tokens in documents to constrain\n",
      "them to be under a certain limit. By default, when set to None, this will\n",
      "be the same as the embedding model name. However, there are some cases\n",
      "where you may want to use this Embedding class with a model name not\n",
      "supported by tiktoken. This can include when using Azure embeddings or\n",
      "when using one of the many model providers that expose an OpenAI-like\n",
      "API but with different models. In those cases, in order to avoid erroring\n",
      "when tiktoken is called, you can specify a model name to use here.\n",
      "\n",
      "param top_logprobs: int | None = None#\n",
      "\n",
      "Number of most likely tokens to return at each token position, each with\n",
      "an associated log probability. logprobs must be set to true\n",
      "if this parameter is used.\n",
      "\n",
      "param top_p: float | None = None#\n",
      "\n",
      "Total probability mass of tokens to consider at each step.\n",
      "\n",
      "param truncation: str | None = None#\n",
      "\n",
      "Truncation strategy (Responses API). Can be 'auto' or 'disabled'\n",
      "(default). If 'auto', model may drop input items from the middle of the\n",
      "message sequence to fit the context window.\n",
      "\n",
      "Added in version 0.3.24.\n",
      "\n",
      "param use_previous_response_id: bool = False#\n",
      "\n",
      "If True, always pass previous_response_id using the ID of the most recent\n",
      "response. Responses API only.\n",
      "\n",
      "Input messages up to the most recent response will be dropped from request\n",
      "payloads.\n",
      "\n",
      "For example, the following two are equivalent:\n",
      "\n",
      "```python\n",
      "llm = ChatOpenAI(\n",
      "    model=\"o4-mini\",\n",
      "    use_previous_response_id=True,\n",
      ")\n",
      "llm.invoke(\n",
      "    [\n",
      "        HumanMessage(\"Hello\"),\n",
      "        AIMessage(\"Hi there!\", response_metadata={\"id\": \"resp_123\"}),\n",
      "        HumanMessage(\"How are you?\"),\n",
      "    ]\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "```python\n",
      "llm = ChatOpenAI(\n",
      "    model=\"o4-mini\",\n",
      "    use_responses_api=True,\n",
      ")\n",
      "llm.invoke([HumanMessage(\"How are you?\")], previous_response_id=\"resp_123\")\n",
      "\n",
      "```\n",
      "\n",
      "Added in version 0.3.26.\n",
      "\n",
      "param use_responses_api: bool | None = None#\n",
      "\n",
      "Whether to use the Responses API instead of the Chat API.\n",
      "\n",
      "If not specified then will be inferred based on invocation params.\n",
      "\n",
      "Added in version 0.3.9.\n",
      "\n",
      "param verbose: bool [Optional]#\n",
      "\n",
      "Whether to print out response text.\n",
      "\n",
      "__call__(\n",
      "\n",
      "_messages: list[BaseMessage]_,\n",
      "_stop: list[str] | None = None_,\n",
      "_callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)[#](#langchain_deepseek.chat_models.ChatDeepSeek.__call__)\n",
      "\n",
      "Deprecated since version 0.1.7: Use [invoke()](#langchain_deepseek.chat_models.ChatDeepSeek.invoke) instead. It will not be removed until langchain-core==1.0.\n",
      "\n",
      "Call the model.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **messages** (_list__[_[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)_]_) – List of messages.\n",
      "\n",
      "- **stop** (_list__[__str__] __| __None_) – Stop words to use when generating. Model output is cut off at the\n",
      "first occurrence of any of these substrings.\n",
      "\n",
      "- **callbacks** (_list__[_[BaseCallbackHandler](../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler)_] __| _[BaseCallbackManager](../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager)_ | __None_) – Callbacks to pass through. Used for executing additional\n",
      "functionality, such as logging or streaming, throughout generation.\n",
      "\n",
      "- ****kwargs** (_Any_) – Arbitrary additional keyword arguments. These are usually passed\n",
      "to the model provider API call.\n",
      "\n",
      "Returns:\n",
      "\n",
      "The model output message.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)\n",
      "\n",
      "async abatch(\n",
      "\n",
      "_inputs: list[Input]_,\n",
      "_config: RunnableConfig | list[RunnableConfig] | None = None_,\n",
      "_*_,\n",
      "_return_exceptions: bool = False_,\n",
      "_**kwargs: Any | None_,\n",
      "\n",
      ") → list[Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.abatch)\n",
      "Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      "\n",
      "The default implementation of batch works well for IO bound runnables.\n",
      "\n",
      "Subclasses should override this method if they can batch more efficiently;\n",
      "e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **inputs** (_list__[__Input__]_) – A list of inputs to the Runnable.\n",
      "\n",
      "- **config** ([RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_ | __list__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_] __| __None_) – A config to use when invoking the Runnable.\n",
      "The config supports standard keys like 'tags', 'metadata' for\n",
      "tracing purposes, 'max_concurrency' for controlling how much work to\n",
      "do in parallel, and other keys. Please refer to the RunnableConfig\n",
      "for more details. Defaults to None.\n",
      "\n",
      "- **return_exceptions** (_bool_) – Whether to return exceptions instead of raising them.\n",
      "Defaults to False.\n",
      "\n",
      "- **kwargs** (_Any__ | __None_) – Additional keyword arguments to pass to the Runnable.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A list of outputs from the Runnable.\n",
      "\n",
      "Return type:\n",
      "\n",
      "list[_Output_]\n",
      "\n",
      "async abatch_as_completed(\n",
      "\n",
      "_inputs: Sequence[Input]_,\n",
      "_config: RunnableConfig | Sequence[RunnableConfig] | None = None_,\n",
      "_*_,\n",
      "_return_exceptions: bool = False_,\n",
      "_**kwargs: Any | None_,\n",
      "\n",
      ") → AsyncIterator[tuple[int, Output | Exception]][#](#langchain_deepseek.chat_models.ChatDeepSeek.abatch_as_completed)\n",
      "Run ainvoke in parallel on a list of inputs.\n",
      "\n",
      "Yields results as they complete.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **inputs** (_Sequence__[__Input__]_) – A list of inputs to the Runnable.\n",
      "\n",
      "- **config** ([RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_ | __Sequence__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_] __| __None_) – A config to use when invoking the Runnable.\n",
      "The config supports standard keys like 'tags', 'metadata' for\n",
      "tracing purposes, 'max_concurrency' for controlling how much work to\n",
      "do in parallel, and other keys. Please refer to the RunnableConfig\n",
      "for more details. Defaults to None.\n",
      "\n",
      "- **return_exceptions** (_bool_) – Whether to return exceptions instead of raising them.\n",
      "Defaults to False.\n",
      "\n",
      "- **kwargs** (_Any__ | __None_) – Additional keyword arguments to pass to the Runnable.\n",
      "\n",
      "Yields:\n",
      "\n",
      "A tuple of the index of the input and the output from the Runnable.\n",
      "\n",
      "Return type:\n",
      "\n",
      "_AsyncIterator_[tuple[int, _Output_ | Exception]]\n",
      "\n",
      "async ainvoke(\n",
      "\n",
      "_input: LanguageModelInput_,\n",
      "_config: RunnableConfig | None = None_,\n",
      "_*_,\n",
      "_stop: list[str] | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)[#](#langchain_deepseek.chat_models.ChatDeepSeek.ainvoke)\n",
      "Default implementation of ainvoke, calls invoke from a thread.\n",
      "\n",
      "The default implementation allows usage of async code even if\n",
      "the Runnable did not implement a native async version of invoke.\n",
      "\n",
      "Subclasses should override this method if they can run asynchronously.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **input** (_LanguageModelInput_)\n",
      "\n",
      "- **config** (_Optional__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]_)\n",
      "\n",
      "- **stop** (_Optional__[__list__[__str__]__]_)\n",
      "\n",
      "- **kwargs** (_Any_)\n",
      "\n",
      "Return type:\n",
      "\n",
      "[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)\n",
      "\n",
      "async astream(\n",
      "\n",
      "_input: LanguageModelInput_,\n",
      "_config: RunnableConfig | None = None_,\n",
      "_*_,\n",
      "_stop: list[str] | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → AsyncIterator[[BaseMessageChunk](../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk)][#](#langchain_deepseek.chat_models.ChatDeepSeek.astream)\n",
      "Default implementation of astream, which calls ainvoke.\n",
      "\n",
      "Subclasses should override this method if they support streaming output.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **input** (_LanguageModelInput_) – The input to the Runnable.\n",
      "\n",
      "- **config** (_Optional__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]_) – The config to use for the Runnable. Defaults to None.\n",
      "\n",
      "- **kwargs** (_Any_) – Additional keyword arguments to pass to the Runnable.\n",
      "\n",
      "- **stop** (_Optional__[__list__[__str__]__]_)\n",
      "\n",
      "Yields:\n",
      "\n",
      "The output of the Runnable.\n",
      "\n",
      "Return type:\n",
      "\n",
      "AsyncIterator[[BaseMessageChunk](../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk)]\n",
      "\n",
      "async astream_events(\n",
      "\n",
      "_input: Any_,\n",
      "_config: RunnableConfig | None = None_,\n",
      "_*_,\n",
      "_version: Literal['v1', 'v2'] = 'v2'_,\n",
      "_include_names: Sequence[str] | None = None_,\n",
      "_include_types: Sequence[str] | None = None_,\n",
      "_include_tags: Sequence[str] | None = None_,\n",
      "_exclude_names: Sequence[str] | None = None_,\n",
      "_exclude_types: Sequence[str] | None = None_,\n",
      "_exclude_tags: Sequence[str] | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → AsyncIterator[StreamEvent][#](#langchain_deepseek.chat_models.ChatDeepSeek.astream_events)\n",
      "Generate a stream of events.\n",
      "\n",
      "Use to create an iterator over StreamEvents that provide real-time information\n",
      "about the progress of the Runnable, including StreamEvents from intermediate\n",
      "results.\n",
      "\n",
      "A StreamEvent is a dictionary with the following schema:\n",
      "\n",
      "- event: **str** - Event names are of the format:\n",
      "on_[runnable_type]_(start|stream|end).\n",
      "\n",
      "- name: **str** - The name of the Runnable that generated the event.\n",
      "\n",
      "- run_id: **str** - randomly generated ID associated with the given\n",
      "execution of the Runnable that emitted the event. A child Runnable that gets\n",
      "invoked as part of the execution of a parent Runnable is assigned its own\n",
      "unique ID.\n",
      "\n",
      "- parent_ids: **list[str]** - The IDs of the parent runnables that generated\n",
      "the event. The root Runnable will have an empty list. The order of the parent\n",
      "IDs is from the root to the immediate parent. Only available for v2 version of\n",
      "the API. The v1 version of the API will return an empty list.\n",
      "\n",
      "- tags: **Optional[list[str]]** - The tags of the Runnable that generated\n",
      "the event.\n",
      "\n",
      "- metadata: **Optional[dict[str, Any]]** - The metadata of the Runnable that\n",
      "generated the event.\n",
      "\n",
      "- data: **dict[str, Any]**\n",
      "\n",
      "Below is a table that illustrates some events that might be emitted by various\n",
      "chains. Metadata fields have been omitted from the table for brevity.\n",
      "Chain definitions have been included after the table.\n",
      "\n",
      "Note\n",
      "\n",
      "This reference table is for the V2 version of the schema.\n",
      "\n",
      "| event | name | chunk | input | output |\n",
      "| ---- | ---- | ---- | ---- | ---- |\n",
      "| on_chat_model_start | [model name] |  | {“messages”: [[SystemMessage, HumanMessage]]} |  |\n",
      "| on_chat_model_stream | [model name] | AIMessageChunk(content=”hello”) |  |  |\n",
      "| on_chat_model_end | [model name] |  | {“messages”: [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=”hello world”) |\n",
      "| on_llm_start | [model name] |  | {‘input’: ‘hello’} |  |\n",
      "| on_llm_stream | [model name] | ‘Hello’ |  |  |\n",
      "| on_llm_end | [model name] |  | ‘Hello human!’ |  |\n",
      "| on_chain_start | format_docs |  |  |  |\n",
      "| on_chain_stream | format_docs | “hello world!, goodbye world!” |  |  |\n",
      "| on_chain_end | format_docs |  | [Document(…)] | “hello world!, goodbye world!” |\n",
      "| on_tool_start | some_tool |  | {“x”: 1, “y”: “2”} |  |\n",
      "| on_tool_end | some_tool |  |  | {“x”: 1, “y”: “2”} |\n",
      "| on_retriever_start | [retriever name] |  | {“query”: “hello”} |  |\n",
      "| on_retriever_end | [retriever name] |  | {“query”: “hello”} | [Document(…), ..] |\n",
      "| on_prompt_start | [template_name] |  | {“question”: “hello”} |  |\n",
      "| on_prompt_end | [template_name] |  | {“question”: “hello”} | ChatPromptValue(messages: [SystemMessage, …]) |\n",
      "\n",
      "In addition to the standard events, users can also dispatch custom events (see example below).\n",
      "\n",
      "Custom events will be only be surfaced with in the v2 version of the API!\n",
      "\n",
      "A custom event has following format:\n",
      "\n",
      "| Attribute | Type | Description |\n",
      "| ---- | ---- | ---- |\n",
      "| name | str | A user defined name for the event. |\n",
      "| data | Any | The data associated with the event. This can be anything, though we suggest making it JSON serializable. |\n",
      "\n",
      "Here are declarations associated with the standard events shown above:\n",
      "\n",
      "format_docs:\n",
      "\n",
      "```python\n",
      "def format_docs(docs: list[Document]) -> str:\n",
      "    '''Format the docs.'''\n",
      "    return \", \".join([doc.page_content for doc in docs])\n",
      "\n",
      "format_docs = RunnableLambda(format_docs)\n",
      "\n",
      "```\n",
      "\n",
      "some_tool:\n",
      "\n",
      "```python\n",
      "@tool\n",
      "def some_tool(x: int, y: str) -> dict:\n",
      "    '''Some_tool.'''\n",
      "    return {\"x\": x, \"y\": y}\n",
      "\n",
      "```\n",
      "\n",
      "prompt:\n",
      "\n",
      "```python\n",
      "template = ChatPromptTemplate.from_messages(\n",
      "    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      ").with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      "\n",
      "```\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "from langchain_core.runnables import RunnableLambda\n",
      "\n",
      "async def reverse(s: str) -> str:\n",
      "    return s[::-1]\n",
      "\n",
      "chain = RunnableLambda(func=reverse)\n",
      "\n",
      "events = [\n",
      "    event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      "]\n",
      "\n",
      "# will produce the following events (run_id, and parent_ids\n",
      "# has been omitted for brevity):\n",
      "[\n",
      "    {\n",
      "        \"data\": {\"input\": \"hello\"},\n",
      "        \"event\": \"on_chain_start\",\n",
      "        \"metadata\": {},\n",
      "        \"name\": \"reverse\",\n",
      "        \"tags\": [],\n",
      "    },\n",
      "    {\n",
      "        \"data\": {\"chunk\": \"olleh\"},\n",
      "        \"event\": \"on_chain_stream\",\n",
      "        \"metadata\": {},\n",
      "        \"name\": \"reverse\",\n",
      "        \"tags\": [],\n",
      "    },\n",
      "    {\n",
      "        \"data\": {\"output\": \"olleh\"},\n",
      "        \"event\": \"on_chain_end\",\n",
      "        \"metadata\": {},\n",
      "        \"name\": \"reverse\",\n",
      "        \"tags\": [],\n",
      "    },\n",
      "]\n",
      "\n",
      "```\n",
      "\n",
      "Example: Dispatch Custom Event\n",
      "\n",
      "```python\n",
      "from langchain_core.callbacks.manager import (\n",
      "    adispatch_custom_event,\n",
      ")\n",
      "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      "import asyncio\n",
      "\n",
      "async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      "    \"\"\"Do something that takes a long time.\"\"\"\n",
      "    await asyncio.sleep(1) # Placeholder for some slow operation\n",
      "    await adispatch_custom_event(\n",
      "        \"progress_event\",\n",
      "        {\"message\": \"Finished step 1 of 3\"},\n",
      "        config=config # Must be included for python < 3.10\n",
      "    )\n",
      "    await asyncio.sleep(1) # Placeholder for some slow operation\n",
      "    await adispatch_custom_event(\n",
      "        \"progress_event\",\n",
      "        {\"message\": \"Finished step 2 of 3\"},\n",
      "        config=config # Must be included for python < 3.10\n",
      "    )\n",
      "    await asyncio.sleep(1) # Placeholder for some slow operation\n",
      "    return \"Done\"\n",
      "\n",
      "slow_thing = RunnableLambda(slow_thing)\n",
      "\n",
      "async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      "    print(event)\n",
      "\n",
      "```\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **input** (_Any_) – The input to the Runnable.\n",
      "\n",
      "- **config** (_Optional__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]_) – The config to use for the Runnable.\n",
      "\n",
      "- **version** (_Literal__[__'v1'__, __'v2'__]_) – The version of the schema to use either v2 or v1.\n",
      "Users should use v2.\n",
      "v1 is for backwards compatibility and will be deprecated\n",
      "in 0.4.0.\n",
      "No default will be assigned until the API is stabilized.\n",
      "custom events will only be surfaced in v2.\n",
      "\n",
      "- **include_names** (_Optional__[__Sequence__[__str__]__]_) – Only include events from runnables with matching names.\n",
      "\n",
      "- **include_types** (_Optional__[__Sequence__[__str__]__]_) – Only include events from runnables with matching types.\n",
      "\n",
      "- **include_tags** (_Optional__[__Sequence__[__str__]__]_) – Only include events from runnables with matching tags.\n",
      "\n",
      "- **exclude_names** (_Optional__[__Sequence__[__str__]__]_) – Exclude events from runnables with matching names.\n",
      "\n",
      "- **exclude_types** (_Optional__[__Sequence__[__str__]__]_) – Exclude events from runnables with matching types.\n",
      "\n",
      "- **exclude_tags** (_Optional__[__Sequence__[__str__]__]_) – Exclude events from runnables with matching tags.\n",
      "\n",
      "- **kwargs** (_Any_) – Additional keyword arguments to pass to the Runnable.\n",
      "These will be passed to astream_log as this implementation\n",
      "of astream_events is built on top of astream_log.\n",
      "\n",
      "Yields:\n",
      "\n",
      "An async stream of StreamEvents.\n",
      "\n",
      "Raises:\n",
      "\n",
      "**NotImplementedError** – If the version is not v1 or v2.\n",
      "\n",
      "Return type:\n",
      "\n",
      "AsyncIterator[StreamEvent]\n",
      "\n",
      "batch(\n",
      "\n",
      "_inputs: list[Input]_,\n",
      "_config: RunnableConfig | list[RunnableConfig] | None = None_,\n",
      "_*_,\n",
      "_return_exceptions: bool = False_,\n",
      "_**kwargs: Any | None_,\n",
      "\n",
      ") → list[Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.batch)\n",
      "Default implementation runs invoke in parallel using a thread pool executor.\n",
      "\n",
      "The default implementation of batch works well for IO bound runnables.\n",
      "\n",
      "Subclasses should override this method if they can batch more efficiently;\n",
      "e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **inputs** (_list__[__Input__]_)\n",
      "\n",
      "- **config** ([RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_ | __list__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_] __| __None_)\n",
      "\n",
      "- **return_exceptions** (_bool_)\n",
      "\n",
      "- **kwargs** (_Any__ | __None_)\n",
      "\n",
      "Return type:\n",
      "\n",
      "list[_Output_]\n",
      "\n",
      "batch_as_completed(\n",
      "\n",
      "_inputs: Sequence[Input]_,\n",
      "_config: RunnableConfig | Sequence[RunnableConfig] | None = None_,\n",
      "_*_,\n",
      "_return_exceptions: bool = False_,\n",
      "_**kwargs: Any | None_,\n",
      "\n",
      ") → Iterator[tuple[int, Output | Exception]][#](#langchain_deepseek.chat_models.ChatDeepSeek.batch_as_completed)\n",
      "Run invoke in parallel on a list of inputs.\n",
      "\n",
      "Yields results as they complete.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **inputs** (_Sequence__[__Input__]_)\n",
      "\n",
      "- **config** ([RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_ | __Sequence__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_] __| __None_)\n",
      "\n",
      "- **return_exceptions** (_bool_)\n",
      "\n",
      "- **kwargs** (_Any__ | __None_)\n",
      "\n",
      "Return type:\n",
      "\n",
      "_Iterator_[tuple[int, _Output_ | Exception]]\n",
      "\n",
      "bind(\n",
      "\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.bind)\n",
      "Bind arguments to a Runnable, returning a new Runnable.\n",
      "\n",
      "Useful when a Runnable in a chain requires an argument that is not\n",
      "in the output of the previous Runnable or included in the user input.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "**kwargs** (_Any_) – The arguments to bind to the Runnable.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the arguments bound.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[_Input_, _Output_]\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "from langchain_ollama import ChatOllama\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "\n",
      "llm = ChatOllama(model='llama2')\n",
      "\n",
      "# Without bind.\n",
      "chain = (\n",
      "    llm\n",
      "    | StrOutputParser()\n",
      ")\n",
      "\n",
      "chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      "# Output is 'One two three four five.'\n",
      "\n",
      "# With bind.\n",
      "chain = (\n",
      "    llm.bind(stop=[\"three\"])\n",
      "    | StrOutputParser()\n",
      ")\n",
      "\n",
      "chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      "# Output is 'One two'\n",
      "\n",
      "```\n",
      "\n",
      "bind_functions(\n",
      "\n",
      "_functions: Sequence[dict[str, Any] | type[BaseModel] | Callable | BaseTool]_,\n",
      "_function_call: _FunctionCall | str | Literal['auto', 'none'] | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[[PromptValue](../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue) | str | Sequence[[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage) | list[str] | tuple[str, str] | str | dict[str, Any]], [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)][#](#langchain_deepseek.chat_models.ChatDeepSeek.bind_functions)\n",
      "\n",
      "Deprecated since version 0.2.1: Use [bind_tools()](../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind_tools) instead. It will not be removed until langchain-openai==1.0.0.\n",
      "\n",
      "Bind functions (and other objects) to this chat model.\n",
      "\n",
      "Assumes model is compatible with OpenAI function-calling API.\n",
      "\n",
      "Note\n",
      "\n",
      "Using bind_tools() is recommended instead, as the functions and\n",
      "function_call request parameters are officially marked as deprecated by\n",
      "OpenAI.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **functions** (_Sequence__[__dict__[__str__, __Any__] __| __type__[__BaseModel__] __| __Callable__ | _[BaseTool](../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool)_]_) – A list of function definitions to bind to this chat model.\n",
      "Can be  a dictionary, pydantic model, or callable. Pydantic\n",
      "models and callables will be automatically converted to\n",
      "their schema dictionary representation.\n",
      "\n",
      "- **function_call** (__FunctionCall__ | __str__ | __Literal__[__'auto'__, __'none'__] __| __None_) – Which function to require the model to call.\n",
      "Must be the name of the single provided function or\n",
      "'auto' to automatically determine which function to call\n",
      "(if any).\n",
      "\n",
      "- ****kwargs** (_Any_) – Any additional parameters to pass to the\n",
      "Runnable constructor.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[[PromptValue](../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue) | str | _Sequence_[[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage) | list[str] | tuple[str, str] | str | dict[str, _Any_]], [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)]\n",
      "\n",
      "bind_tools(\n",
      "\n",
      "_tools: Sequence[dict[str, Any] | type | Callable | BaseTool]_,\n",
      "_*_,\n",
      "_tool_choice: dict | str | Literal['auto', 'none', 'required', 'any'] | bool | None = None_,\n",
      "_strict: bool | None = None_,\n",
      "_parallel_tool_calls: bool | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[[PromptValue](../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue) | str | Sequence[[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage) | list[str] | tuple[str, str] | str | dict[str, Any]], [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)][#](#langchain_deepseek.chat_models.ChatDeepSeek.bind_tools)\n",
      "Bind tool-like objects to this chat model.\n",
      "\n",
      "Assumes model is compatible with OpenAI tool-calling API.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **tools** (_Sequence__[__dict__[__str__, __Any__] __| __type__ | __Callable__ | _[BaseTool](../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool)_]_) – A list of tool definitions to bind to this chat model.\n",
      "Supports any tool definition handled by\n",
      "[langchain_core.utils.function_calling.convert_to_openai_tool()](../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool).\n",
      "\n",
      "- **tool_choice** (_dict__ | __str__ | __Literal__[__'auto'__, __'none'__, __'required'__, __'any'__] __| __bool__ | __None_) – \n",
      "\n",
      "Which tool to require the model to call. Options are:\n",
      "\n",
      "- str of the form '<<tool_name>>': calls <<tool_name>> tool.\n",
      "\n",
      "- 'auto': automatically selects a tool (including no tool).\n",
      "\n",
      "- 'none': does not call a tool.\n",
      "\n",
      "- 'any' or 'required' or True: force at least one tool to be called.\n",
      "\n",
      "- dict of the form {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}: calls <<tool_name>> tool.\n",
      "\n",
      "- False or None: no effect, default OpenAI behavior.\n",
      "\n",
      "- **strict** (_bool__ | __None_) – If True, model output is guaranteed to exactly match the JSON Schema\n",
      "provided in the tool definition. The input schema will also be validated according to the\n",
      "[supported schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas?api-mode=responses#supported-schemas).\n",
      "If False, input schema will not be validated and model output will not\n",
      "be validated.\n",
      "If None, strict argument will not be passed to the model.\n",
      "\n",
      "- **parallel_tool_calls** (_bool__ | __None_) – Set to False to disable parallel tool use.\n",
      "Defaults to None (no specification, which allows parallel tool use).\n",
      "\n",
      "- **kwargs** (_Any_) – Any additional parameters are passed directly to\n",
      "[bind()](../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind).\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[[PromptValue](../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue) | str | _Sequence_[[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage) | list[str] | tuple[str, str] | str | dict[str, _Any_]], [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)]\n",
      "\n",
      "Changed in version 0.1.21: Support for strict argument added.\n",
      "\n",
      "configurable_alternatives(\n",
      "\n",
      "_which: ConfigurableField_,\n",
      "_*_,\n",
      "_default_key: str = 'default'_,\n",
      "_prefix_keys: bool = False_,\n",
      "_**kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]_,\n",
      "\n",
      ") → [RunnableSerializable](../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable)[#](#langchain_deepseek.chat_models.ChatDeepSeek.configurable_alternatives)\n",
      "Configure alternatives for Runnables that can be set at runtime.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **which** ([ConfigurableField](../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField)) – The ConfigurableField instance that will be used to select the\n",
      "alternative.\n",
      "\n",
      "- **default_key** (_str_) – The default key to use if no alternative is selected.\n",
      "Defaults to 'default'.\n",
      "\n",
      "- **prefix_keys** (_bool_) – Whether to prefix the keys with the ConfigurableField id.\n",
      "Defaults to False.\n",
      "\n",
      "- ****kwargs** ([Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)_[__Input__, __Output__] __| __Callable__[__[__]__, _[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)_[__Input__, __Output__]__]_) – A dictionary of keys to Runnable instances or callables that\n",
      "return Runnable instances.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the alternatives configured.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[RunnableSerializable](../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable)\n",
      "\n",
      "```python\n",
      "from langchain_anthropic import ChatAnthropic\n",
      "from langchain_core.runnables.utils import ConfigurableField\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatAnthropic(\n",
      "    model_name=\"claude-3-7-sonnet-20250219\"\n",
      ").configurable_alternatives(\n",
      "    ConfigurableField(id=\"llm\"),\n",
      "    default_key=\"anthropic\",\n",
      "    openai=ChatOpenAI()\n",
      ")\n",
      "\n",
      "# uses the default model ChatAnthropic\n",
      "print(model.invoke(\"which organization created you?\").content)\n",
      "\n",
      "# uses ChatOpenAI\n",
      "print(\n",
      "    model.with_config(\n",
      "        configurable={\"llm\": \"openai\"}\n",
      "    ).invoke(\"which organization created you?\").content\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "configurable_fields(\n",
      "\n",
      "_**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption_,\n",
      "\n",
      ") → [RunnableSerializable](../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable)[#](#langchain_deepseek.chat_models.ChatDeepSeek.configurable_fields)\n",
      "Configure particular Runnable fields at runtime.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "****kwargs** ([ConfigurableField](../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField)_ | _[ConfigurableFieldSingleOption](../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption)_ | _[ConfigurableFieldMultiOption](../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption)) – A dictionary of ConfigurableField instances to configure.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the fields configured.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[RunnableSerializable](../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable)\n",
      "\n",
      "```python\n",
      "from langchain_core.runnables import ConfigurableField\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      "    max_tokens=ConfigurableField(\n",
      "        id=\"output_token_number\",\n",
      "        name=\"Max tokens in the output\",\n",
      "        description=\"The maximum number of tokens in the output\",\n",
      "    )\n",
      ")\n",
      "\n",
      "# max_tokens = 20\n",
      "print(\n",
      "    \"max_tokens_20: \",\n",
      "    model.invoke(\"tell me something about chess\").content\n",
      ")\n",
      "\n",
      "# max_tokens = 200\n",
      "print(\"max_tokens_200: \", model.with_config(\n",
      "    configurable={\"output_token_number\": 200}\n",
      "    ).invoke(\"tell me something about chess\").content\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "get_num_tokens(text: str) → int#\n",
      "\n",
      "Get the number of tokens present in the text.\n",
      "\n",
      "Useful for checking if an input fits in a model’s context window.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "**text** (_str_) – The string input to tokenize.\n",
      "\n",
      "Returns:\n",
      "\n",
      "The integer number of tokens in the text.\n",
      "\n",
      "Return type:\n",
      "\n",
      "int\n",
      "\n",
      "get_num_tokens_from_messages(\n",
      "\n",
      "_messages: list[BaseMessage]_,\n",
      "_tools: Sequence[dict[str, Any] | type | Callable | BaseTool] | None = None_,\n",
      "\n",
      ") → int[#](#langchain_deepseek.chat_models.ChatDeepSeek.get_num_tokens_from_messages)\n",
      "Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
      "\n",
      "**Requirements**: You must have the pillow installed if you want to count\n",
      "image tokens if you are specifying the image as a base64 string, and you must\n",
      "have both pillow and httpx installed if you are specifying the image\n",
      "as a URL. If these aren’t installed image inputs will be ignored in token\n",
      "counting.\n",
      "\n",
      "[OpenAI reference](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **messages** (_list__[_[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)_]_) – The message inputs to tokenize.\n",
      "\n",
      "- **tools** (_Sequence__[__dict__[__str__, __Any__] __| __type__ | __Callable__ | _[BaseTool](../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool)_] __| __None_) – If provided, sequence of dict, BaseModel, function, or BaseTools\n",
      "to be converted to tool schemas.\n",
      "\n",
      "Return type:\n",
      "\n",
      "int\n",
      "\n",
      "get_token_ids(text: str) → list[int]#\n",
      "\n",
      "Get the tokens present in the text with tiktoken package.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "**text** (_str_)\n",
      "\n",
      "Return type:\n",
      "\n",
      "list[int]\n",
      "\n",
      "invoke(\n",
      "\n",
      "_input: LanguageModelInput_,\n",
      "_config: RunnableConfig | None = None_,\n",
      "_*_,\n",
      "_stop: list[str] | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)[#](#langchain_deepseek.chat_models.ChatDeepSeek.invoke)\n",
      "Transform a single input into an output.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **input** (_LanguageModelInput_) – The input to the Runnable.\n",
      "\n",
      "- **config** (_Optional__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]_) – A config to use when invoking the Runnable.\n",
      "The config supports standard keys like 'tags', 'metadata' for\n",
      "tracing purposes, 'max_concurrency' for controlling how much work to\n",
      "do in parallel, and other keys. Please refer to the RunnableConfig\n",
      "for more details. Defaults to None.\n",
      "\n",
      "- **stop** (_Optional__[__list__[__str__]__]_)\n",
      "\n",
      "- **kwargs** (_Any_)\n",
      "\n",
      "Returns:\n",
      "\n",
      "The output of the Runnable.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage)\n",
      "\n",
      "stream(\n",
      "\n",
      "_input: LanguageModelInput_,\n",
      "_config: RunnableConfig | None = None_,\n",
      "_*_,\n",
      "_stop: list[str] | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → Iterator[[BaseMessageChunk](../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk)][#](#langchain_deepseek.chat_models.ChatDeepSeek.stream)\n",
      "Default implementation of stream, which calls invoke.\n",
      "\n",
      "Subclasses should override this method if they support streaming output.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **input** (_LanguageModelInput_) – The input to the Runnable.\n",
      "\n",
      "- **config** (_Optional__[_[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]_) – The config to use for the Runnable. Defaults to None.\n",
      "\n",
      "- **kwargs** (_Any_) – Additional keyword arguments to pass to the Runnable.\n",
      "\n",
      "- **stop** (_Optional__[__list__[__str__]__]_)\n",
      "\n",
      "Yields:\n",
      "\n",
      "The output of the Runnable.\n",
      "\n",
      "Return type:\n",
      "\n",
      "Iterator[[BaseMessageChunk](../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk)]\n",
      "\n",
      "with_alisteners(\n",
      "\n",
      "_*_,\n",
      "_on_start: AsyncListener | None = None_,\n",
      "_on_end: AsyncListener | None = None_,\n",
      "_on_error: AsyncListener | None = None_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.with_alisteners)\n",
      "Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n",
      "\n",
      "The Run object contains information about the run, including its id,\n",
      "type, input, output, error, start_time, end_time, and any tags or metadata\n",
      "added to the run.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **on_start** (_Optional__[__AsyncListener__]_) – Called asynchronously before the Runnable starts running,\n",
      "with the Run object. Defaults to None.\n",
      "\n",
      "- **on_end** (_Optional__[__AsyncListener__]_) – Called asynchronously after the Runnable finishes running,\n",
      "with the Run object. Defaults to None.\n",
      "\n",
      "- **on_error** (_Optional__[__AsyncListener__]_) – Called asynchronously if the Runnable throws an error,\n",
      "with the Run object. Defaults to None.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the listeners bound.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output]\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "from langchain_core.runnables import RunnableLambda, Runnable\n",
      "from datetime import datetime, timezone\n",
      "import time\n",
      "import asyncio\n",
      "\n",
      "def format_t(timestamp: float) -> str:\n",
      "    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
      "\n",
      "async def test_runnable(time_to_sleep : int):\n",
      "    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      "    await asyncio.sleep(time_to_sleep)\n",
      "    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      "\n",
      "async def fn_start(run_obj : Runnable):\n",
      "    print(f\"on start callback starts at {format_t(time.time())}\")\n",
      "    await asyncio.sleep(3)\n",
      "    print(f\"on start callback ends at {format_t(time.time())}\")\n",
      "\n",
      "async def fn_end(run_obj : Runnable):\n",
      "    print(f\"on end callback starts at {format_t(time.time())}\")\n",
      "    await asyncio.sleep(2)\n",
      "    print(f\"on end callback ends at {format_t(time.time())}\")\n",
      "\n",
      "runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      "    on_start=fn_start,\n",
      "    on_end=fn_end\n",
      ")\n",
      "async def concurrent_runs():\n",
      "    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      "\n",
      "asyncio.run(concurrent_runs())\n",
      "Result:\n",
      "on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
      "on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
      "on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
      "on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
      "Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
      "Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
      "Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
      "on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
      "Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
      "on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
      "on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
      "on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
      "\n",
      "```\n",
      "\n",
      "with_config(\n",
      "\n",
      "_config: RunnableConfig | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.with_config)\n",
      "Bind config to a Runnable, returning a new Runnable.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **config** ([RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_ | __None_) – The config to bind to the Runnable.\n",
      "\n",
      "- **kwargs** (_Any_) – Additional keyword arguments to pass to the Runnable.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the config bound.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[_Input_, _Output_]\n",
      "\n",
      "with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class 'Exception'>,), exception_key: Optional[str] = None) → RunnableWithFallbacksT[Input, Output]#\n",
      "\n",
      "Add fallbacks to a Runnable, returning a new Runnable.\n",
      "\n",
      "The new Runnable will try the original Runnable, and then each fallback\n",
      "in order, upon failures.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **fallbacks** (_Sequence__[_[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)_[__Input__, __Output__]__]_) – A sequence of runnables to try if the original Runnable fails.\n",
      "\n",
      "- **exceptions_to_handle** (_tuple__[__type__[__BaseException__]__, __...__]_) – A tuple of exception types to handle.\n",
      "Defaults to (Exception,).\n",
      "\n",
      "- **exception_key** (_Optional__[__str__]_) – If string is specified then handled exceptions will be passed\n",
      "to fallbacks as part of the input under the specified key. If None,\n",
      "exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      "and its fallbacks must accept a dictionary as input. Defaults to None.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable that will try the original Runnable, and then each\n",
      "fallback in order, upon failures.\n",
      "\n",
      "Return type:\n",
      "\n",
      "RunnableWithFallbacksT[Input, Output]\n",
      "\n",
      "Example\n",
      "\n",
      "```python\n",
      "from typing import Iterator\n",
      "\n",
      "from langchain_core.runnables import RunnableGenerator\n",
      "\n",
      "def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      "    raise ValueError()\n",
      "    yield \"\"\n",
      "\n",
      "def _generate(input: Iterator) -> Iterator[str]:\n",
      "    yield from \"foo bar\"\n",
      "\n",
      "runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      "    [RunnableGenerator(_generate)]\n",
      "    )\n",
      "print(''.join(runnable.stream({}))) #foo bar\n",
      "\n",
      "```\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **fallbacks** (_Sequence__[_[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)_[__Input__, __Output__]__]_) – A sequence of runnables to try if the original Runnable fails.\n",
      "\n",
      "- **exceptions_to_handle** (_tuple__[__type__[__BaseException__]__, __...__]_) – A tuple of exception types to handle.\n",
      "\n",
      "- **exception_key** (_Optional__[__str__]_) – If string is specified then handled exceptions will be passed\n",
      "to fallbacks as part of the input under the specified key. If None,\n",
      "exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      "and its fallbacks must accept a dictionary as input.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable that will try the original Runnable, and then each\n",
      "fallback in order, upon failures.\n",
      "\n",
      "Return type:\n",
      "\n",
      "RunnableWithFallbacksT[Input, Output]\n",
      "\n",
      "with_listeners(\n",
      "\n",
      "_*_,\n",
      "_on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None_,\n",
      "_on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None_,\n",
      "_on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.with_listeners)\n",
      "Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      "\n",
      "The Run object contains information about the run, including its id,\n",
      "type, input, output, error, start_time, end_time, and any tags or metadata\n",
      "added to the run.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **on_start** (_Optional__[__Union__[__Callable__[__[__Run__]__, __None__]__, __Callable__[__[__Run__, _[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]__, __None__]__]__]_) – Called before the Runnable starts running, with the Run object.\n",
      "Defaults to None.\n",
      "\n",
      "- **on_end** (_Optional__[__Union__[__Callable__[__[__Run__]__, __None__]__, __Callable__[__[__Run__, _[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]__, __None__]__]__]_) – Called after the Runnable finishes running, with the Run object.\n",
      "Defaults to None.\n",
      "\n",
      "- **on_error** (_Optional__[__Union__[__Callable__[__[__Run__]__, __None__]__, __Callable__[__[__Run__, _[RunnableConfig](../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)_]__, __None__]__]__]_) – Called if the Runnable throws an error, with the Run object.\n",
      "Defaults to None.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the listeners bound.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output]\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "from langchain_core.runnables import RunnableLambda\n",
      "from langchain_core.tracers.schemas import Run\n",
      "\n",
      "import time\n",
      "\n",
      "def test_runnable(time_to_sleep : int):\n",
      "    time.sleep(time_to_sleep)\n",
      "\n",
      "def fn_start(run_obj: Run):\n",
      "    print(\"start_time:\", run_obj.start_time)\n",
      "\n",
      "def fn_end(run_obj: Run):\n",
      "    print(\"end_time:\", run_obj.end_time)\n",
      "\n",
      "chain = RunnableLambda(test_runnable).with_listeners(\n",
      "    on_start=fn_start,\n",
      "    on_end=fn_end\n",
      ")\n",
      "chain.invoke(2)\n",
      "\n",
      "```\n",
      "\n",
      "with_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class 'Exception'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) → Runnable[Input, Output]#\n",
      "\n",
      "Create a new Runnable that retries the original Runnable on exceptions.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **retry_if_exception_type** (_tuple__[__type__[__BaseException__]__, __...__]_) – A tuple of exception types to retry on.\n",
      "Defaults to (Exception,).\n",
      "\n",
      "- **wait_exponential_jitter** (_bool_) – Whether to add jitter to the wait\n",
      "time between retries. Defaults to True.\n",
      "\n",
      "- **stop_after_attempt** (_int_) – The maximum number of attempts to make before\n",
      "giving up. Defaults to 3.\n",
      "\n",
      "- **exponential_jitter_params** (_Optional__[_[ExponentialJitterParams](../../core/runnables/langchain_core.runnables.retry.ExponentialJitterParams.html#langchain_core.runnables.retry.ExponentialJitterParams)_]_) – Parameters for\n",
      "tenacity.wait_exponential_jitter. Namely: initial, max,\n",
      "exp_base, and jitter (all float values).\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable that retries the original Runnable on exceptions.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output]\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "from langchain_core.runnables import RunnableLambda\n",
      "\n",
      "count = 0\n",
      "\n",
      "def _lambda(x: int) -> None:\n",
      "    global count\n",
      "    count = count + 1\n",
      "    if x == 1:\n",
      "        raise ValueError(\"x is 1\")\n",
      "    else:\n",
      "         pass\n",
      "\n",
      "runnable = RunnableLambda(_lambda)\n",
      "try:\n",
      "    runnable.with_retry(\n",
      "        stop_after_attempt=2,\n",
      "        retry_if_exception_type=(ValueError,),\n",
      "    ).invoke(1)\n",
      "except ValueError:\n",
      "    pass\n",
      "\n",
      "assert (count == 2)\n",
      "\n",
      "```\n",
      "\n",
      "with_structured_output(\n",
      "\n",
      "_schema: dict[str, Any] | type[_BM] | type | None = None_,\n",
      "_*_,\n",
      "_method: Literal['function_calling', 'json_mode', 'json_schema'] = 'function_calling'_,\n",
      "_include_raw: bool = False_,\n",
      "_strict: bool | None = None_,\n",
      "_**kwargs: Any_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[[PromptValue](../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue) | str | Sequence[[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage) | list[str] | tuple[str, str] | str | dict[str, Any]], dict | _BM][[source]](../../_modules/langchain_deepseek/chat_models.html#ChatDeepSeek.with_structured_output)[#](#langchain_deepseek.chat_models.ChatDeepSeek.with_structured_output)\n",
      "Model wrapper that returns outputs formatted to match the given schema.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **schema** (_dict__[__str__, __Any__] __| __type__[___BM__] __| __type__ | __None_) – \n",
      "\n",
      "The output schema. Can be passed in as:\n",
      "\n",
      "- an OpenAI function/tool schema,\n",
      "\n",
      "- a JSON Schema,\n",
      "\n",
      "- a TypedDict class (support added in 0.1.20),\n",
      "\n",
      "- or a Pydantic class.\n",
      "\n",
      "If schema is a Pydantic class then the model output will be a\n",
      "Pydantic instance of that class, and the model-generated fields will be\n",
      "validated by the Pydantic class. Otherwise the model output will be a\n",
      "dict and will not be validated. See [langchain_core.utils.function_calling.convert_to_openai_tool()](../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool)\n",
      "for more on how to properly specify types and descriptions of\n",
      "schema fields when specifying a Pydantic or TypedDict class.\n",
      "\n",
      "- **method** (_Literal__[__'function_calling'__, __'json_mode'__, __'json_schema'__]_) – \n",
      "\n",
      "The method for steering model generation, one of:\n",
      "\n",
      "- \n",
      "'function_calling':\n",
      "Uses DeepSeek’s [tool-calling features](https://api-docs.deepseek.com/guides/function_calling).\n",
      "\n",
      "- \n",
      "'json_mode':\n",
      "Uses DeepSeek’s [JSON mode feature](https://api-docs.deepseek.com/guides/json_mode).\n",
      "\n",
      "Changed in version 0.1.3: Added support for 'json_mode'.\n",
      "\n",
      "- **include_raw** (_bool_) – If False then only the parsed structured output is returned. If\n",
      "an error occurs during model output parsing it will be raised. If True\n",
      "then both the raw model response (a BaseMessage) and the parsed model\n",
      "response will be returned. If an error occurs during output parsing it\n",
      "will be caught and returned as well. The final output is always a dict\n",
      "with keys 'raw', 'parsed', and 'parsing_error'.\n",
      "\n",
      "- **strict** (_bool__ | __None_) – Whether to enable strict schema adherence when generating the function\n",
      "call. This parameter is included for compatibility with other chat\n",
      "models, and if specified will be passed to the Chat Completions API\n",
      "in accordance with the OpenAI API specification. However, the DeepSeek\n",
      "API may ignore the parameter.\n",
      "\n",
      "- **kwargs** (_Any_) – Additional keyword args aren’t supported.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A Runnable that takes same inputs as a langchain_core.language_models.chat.BaseChatModel.\n",
      "\n",
      "If include_raw is False and schema is a Pydantic class, Runnable outputs\n",
      "an instance of schema (i.e., a Pydantic object). Otherwise, if include_raw is False then Runnable outputs a dict.\n",
      "\n",
      "If include_raw is True, then Runnable outputs a dict with keys:\n",
      "\n",
      "- 'raw': BaseMessage\n",
      "\n",
      "- 'parsed': None if there was a parsing error, otherwise the type depends on the schema as described above.\n",
      "\n",
      "- 'parsing_error': Optional[BaseException]\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[[PromptValue](../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue) | str | _Sequence_[[BaseMessage](../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage) | list[str] | tuple[str, str] | str | dict[str, _Any_]], dict | __BM_]\n",
      "\n",
      "with_types(\n",
      "\n",
      "_*_,\n",
      "_input_type: type[Input] | None = None_,\n",
      "_output_type: type[Output] | None = None_,\n",
      "\n",
      ") → [Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[Input, Output][#](#langchain_deepseek.chat_models.ChatDeepSeek.with_types)\n",
      "Bind input and output types to a Runnable, returning a new Runnable.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "- **input_type** (_type__[__Input__] __| __None_) – The input type to bind to the Runnable. Defaults to None.\n",
      "\n",
      "- **output_type** (_type__[__Output__] __| __None_) – The output type to bind to the Runnable. Defaults to None.\n",
      "\n",
      "Returns:\n",
      "\n",
      "A new Runnable with the types bound.\n",
      "\n",
      "Return type:\n",
      "\n",
      "[Runnable](../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)[_Input_, _Output_]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 获取当前 notebook 的工作目录（通常是 .ipynb 所在目录）\n",
    "notebook_dir = os.getcwd()\n",
    "target_path = os.path.abspath(os.path.join(notebook_dir, \"../\"))\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "from src.ingest.parsers.langchain_recursive_url import (\n",
    "    langchain_recursive_url_extractor,\n",
    "    langchain_recursive_url_metadata_extractor,\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html#langchain_deepseek.chat_models.ChatDeepSeek\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "metadata = langchain_recursive_url_metadata_extractor(\n",
    "    raw_html=response.text,\n",
    "    url=url,\n",
    "    response=response,\n",
    "    type=\"api\",\n",
    "    lang=\"python\",\n",
    ")\n",
    "print(metadata)\n",
    "doc = langchain_recursive_url_extractor(soup)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a1977",
   "metadata": {},
   "source": [
    "### LangChain Doc Loader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3feddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'source': 'https://python.langchain.com/docs/integrations/chat/', 'title': 'Chat models | 🦜️🔗 LangChain', 'type': 'api', 'lang': 'python'}\n",
      "[](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/chat/index.mdx)# Chat models\n",
      "\n",
      "[Chat models](/docs/concepts/chat_models/) are language models that use a sequence of [messages](/docs/concepts/messages/) as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.\n",
      "\n",
      "infoIf you'd like to write your own chat model, see [this how-to](/docs/how_to/custom_chat_model/).\n",
      "If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).\n",
      "\n",
      " \n",
      "Select [chat model](/docs/integrations/chat/):Google Gemini▾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexity\n",
      "```bash\n",
      "pip install -qU \"langchain[google-genai]\"\n",
      "```\n",
      "\n",
      "```python\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
      "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
      "```\n",
      "\n",
      "```python\n",
      "model.invoke(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "## Featured Providers​\n",
      "\n",
      "infoWhile all these LangChain classes support the indicated advanced feature, you may have\n",
      "to open the provider-specific documentation to learn which hosted models or backends support\n",
      "the feature.\n",
      "\n",
      " \n",
      "| Provider | Tool calling | Structured output | JSON mode | Local | Multimodal | Package |\n",
      "| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
      "| ChatAnthropic | ✅ | ✅ | ❌ | ❌ | ✅ | langchain-anthropic |\n",
      "| ChatMistralAI | ✅ | ✅ | ❌ | ❌ | ❌ | langchain-mistralai |\n",
      "| ChatFireworks | ✅ | ✅ | ✅ | ❌ | ❌ | langchain-fireworks |\n",
      "| AzureChatOpenAI | ✅ | ✅ | ✅ | ❌ | ✅ | langchain-openai |\n",
      "| ChatOpenAI | ✅ | ✅ | ✅ | ❌ | ✅ | langchain-openai |\n",
      "| ChatTogether | ✅ | ✅ | ✅ | ❌ | ❌ | langchain-together |\n",
      "| ChatVertexAI | ✅ | ✅ | ❌ | ❌ | ✅ | langchain-google-vertexai |\n",
      "| ChatGoogleGenerativeAI | ✅ | ✅ | ❌ | ❌ | ✅ | langchain-google-genai |\n",
      "| ChatGroq | ✅ | ✅ | ✅ | ❌ | ❌ | langchain-groq |\n",
      "| ChatCohere | ✅ | ✅ | ❌ | ❌ | ❌ | langchain-cohere |\n",
      "| ChatBedrock | ✅ | ✅ | ❌ | ❌ | ❌ | langchain-aws |\n",
      "| ChatHuggingFace | ✅ | ✅ | ❌ | ✅ | ❌ | langchain-huggingface |\n",
      "| ChatNVIDIA | ✅ | ✅ | ✅ | ✅ | ✅ | langchain-nvidia-ai-endpoints |\n",
      "| ChatOllama | ✅ | ✅ | ✅ | ✅ | ❌ | langchain-ollama |\n",
      "| ChatLlamaCpp | ✅ | ✅ | ❌ | ✅ | ❌ | langchain-community |\n",
      "| ChatAI21 | ✅ | ✅ | ❌ | ❌ | ❌ | langchain-ai21 |\n",
      "| ChatUpstage | ✅ | ✅ | ❌ | ❌ | ❌ | langchain-upstage |\n",
      "| ChatDatabricks | ✅ | ✅ | ❌ | ❌ | ❌ | databricks-langchain |\n",
      "| ChatWatsonx | ✅ | ✅ | ✅ | ❌ | ❌ | langchain-ibm |\n",
      "| ChatXAI | ✅ | ✅ | ❌ | ❌ | ❌ | langchain-xai |\n",
      "| ChatPerplexity | ❌ | ✅ | ✅ | ❌ | ✅ | langchain-perplexity |\n",
      "\n",
      "## All chat models​\n",
      "\n",
      "| Name | Description |\n",
      "| ---- | ---- |\n",
      "| Abso | This will help you get started with ChatAbso chat models. For detaile... |\n",
      "| AI21 Labs | This notebook covers how to get started with AI21 chat models. |\n",
      "| Alibaba Cloud PAI EAS | Alibaba Cloud PAI (Platform for AI) is a lightweight and cost-efficie... |\n",
      "| Anthropic | This notebook provides a quick overview for getting started with Anth... |\n",
      "| Anyscale | This notebook demonstrates the use of langchain.chat_models.ChatAnysc... |\n",
      "| AzureAIChatCompletionsModel | This will help you get started with AzureAIChatCompletionsModel chat ... |\n",
      "| Azure OpenAI | This guide will help you get started with AzureOpenAI chat models. Fo... |\n",
      "| Azure ML Endpoint | Azure Machine Learning is a platform used to build, train, and deploy... |\n",
      "| Baichuan Chat | Baichuan chat models API by Baichuan Intelligent Technology. For more... |\n",
      "| Baidu Qianfan | Baidu AI Cloud Qianfan Platform is a one-stop large model development... |\n",
      "| AWS Bedrock | This doc will help you get started with AWS Bedrock chat models. Amaz... |\n",
      "| Cerebras | This notebook provides a quick overview for getting started with Cere... |\n",
      "| CloudflareWorkersAI | This will help you get started with CloudflareWorkersAI chat models. ... |\n",
      "| Cohere | This notebook covers how to get started with Cohere chat models. |\n",
      "| ContextualAI | This will help you get started with Contextual AI's Grounded Language... |\n",
      "| Coze Chat | ChatCoze chat models API by coze.com. For more information, see https... |\n",
      "| Dappier AI | Dappier: Powering AI with Dynamic, Real-Time Data Models |\n",
      "| Databricks | Databricks Lakehouse Platform unifies data, analytics, and AI on one ... |\n",
      "| DeepInfra | DeepInfra is a serverless inference as a service that provides access... |\n",
      "| DeepSeek | This will help you get started with DeepSeek's hosted chat models. Fo... |\n",
      "| Eden AI | Eden AI is revolutionizing the AI landscape by uniting the best AI pr... |\n",
      "| EverlyAI | EverlyAI allows you to run your ML models at scale in the cloud. It a... |\n",
      "| Featherless AI | This will help you get started with FeatherlessAi chat models. For de... |\n",
      "| Fireworks | This doc helps you get started with Fireworks AI chat models. For det... |\n",
      "| ChatFriendli | Friendli enhances AI application performance and optimizes cost savin... |\n",
      "| Goodfire | This will help you get started with Goodfire chat models. For detaile... |\n",
      "| Google Gemini | Access Google's Generative AI models, including the Gemini family, di... |\n",
      "| Google Cloud Vertex AI | This page provides a quick overview for getting started with VertexAI... |\n",
      "| GPTRouter | GPTRouter is an open source LLM API Gateway that offers a universal A... |\n",
      "| DigitalOcean Gradient | This will help you getting started with DigitalOcean Gradient Chat Mo... |\n",
      "| GreenNode | GreenNode is a global AI solutions provider and a NVIDIA Preferred Pa... |\n",
      "| Groq | This will help you get started with Groq chat models. For detailed do... |\n",
      "| ChatHuggingFace | This will help you get started with langchainhuggingface chat models.... |\n",
      "| IBM watsonx.ai | ChatWatsonx is a wrapper for IBM watsonx.ai foundation models. |\n",
      "| JinaChat | This notebook covers how to get started with JinaChat chat models. |\n",
      "| Kinetica | This notebook demonstrates how to use Kinetica to transform natural l... |\n",
      "| Konko | Konko API is a fully managed Web API designed to help application dev... |\n",
      "| LiteLLM | LiteLLM is a library that simplifies calling Anthropic, Azure, Huggin... |\n",
      "| Llama 2 Chat | This notebook shows how to augment Llama-2 LLMs with the Llama2Chat w... |\n",
      "| Llama API | This notebook shows how to use LangChain with LlamaAPI - a hosted ver... |\n",
      "| LlamaEdge | LlamaEdge allows you to chat with LLMs of GGUF format both locally an... |\n",
      "| Llama.cpp | llama.cpp python library is a simple Python bindings for @ggerganov |\n",
      "| maritalk | MariTalk is an assistant developed by the Brazilian company Maritaca ... |\n",
      "| MiniMax | Minimax is a Chinese startup that provides LLM service for companies ... |\n",
      "| MistralAI | This will help you get started with Mistral chat models. For detailed... |\n",
      "| MLX | This notebook shows how to get started using MLX LLM's as chat models. |\n",
      "| ModelScope | ModelScope (Home | GitHub) is built upon the notion of “Model-as-a-Se... |\n",
      "| Moonshot | Moonshot is a Chinese startup that provides LLM service for companies... |\n",
      "| Naver | This notebook provides a quick overview for getting started with Nave... |\n",
      "| Nebius | This page will help you get started with Nebius AI Studio chat models... |\n",
      "| Netmind | This will help you get started with Netmind chat models. For detailed... |\n",
      "| NVIDIA AI Endpoints | This will help you get started with NVIDIA chat models. For detailed ... |\n",
      "| ChatOCIModelDeployment | This will help you get started with OCIModelDeployment chat models. F... |\n",
      "| OCIGenAI | This notebook provides a quick overview for getting started with OCIG... |\n",
      "| ChatOctoAI | OctoAI offers easy access to efficient compute and enables users to i... |\n",
      "| Ollama | Ollama allows you to run open-source large language models, such as L... |\n",
      "| OpenAI | This notebook provides a quick overview for getting started with Open... |\n",
      "| Outlines | This will help you get started with Outlines chat models. For detaile... |\n",
      "| Perplexity | This page will help you get started with Perplexity chat models. For ... |\n",
      "| Pipeshift | This will help you get started with Pipeshift chat models. For detail... |\n",
      "| ChatPredictionGuard | Prediction Guard is a secure, scalable GenAI platform that safeguards... |\n",
      "| PremAI | PremAI is an all-in-one platform that simplifies the creation of robu... |\n",
      "| PromptLayer ChatOpenAI | This example showcases how to connect to PromptLayer to start recordi... |\n",
      "| Qwen QwQ | This will help you get started with QwQ chat models. For detailed doc... |\n",
      "| Reka | This notebook provides a quick overview for getting started with Reka... |\n",
      "| RunPod Chat Model | Get started with RunPod chat models. |\n",
      "| SambaNovaCloud | This will help you get started with SambaNovaCloud chat models. For d... |\n",
      "| SambaStudio | This will help you get started with SambaStudio chat models. For deta... |\n",
      "| ChatSeekrFlow | Seekr provides AI-powered solutions for structured, explainable, and ... |\n",
      "| Snowflake Cortex | Snowflake Cortex gives you instant access to industry-leading large l... |\n",
      "| solar | Deprecated since version 0.0.34: Use langchain_upstage.ChatUpstage in... |\n",
      "| SparkLLM Chat | SparkLLM chat models API by iFlyTek. For more information, see iFlyTe... |\n",
      "| Nebula (Symbl.ai) | This notebook covers how to get started with Nebula - Symbl.ai's chat... |\n",
      "| Tencent Hunyuan | Tencent's hybrid model API (Hunyuan API) |\n",
      "| Together | This page will help you get started with Together AI chat models. For... |\n",
      "| Tongyi Qwen | Tongyi Qwen is a large language model developed by Alibaba's Damo Aca... |\n",
      "| Upstage | This notebook covers how to get started with Upstage chat models. |\n",
      "| vectara | Vectara is the trusted AI Assistant and Agent platform, which focuses... |\n",
      "| vLLM Chat | vLLM can be deployed as a server that mimics the OpenAI API protocol.... |\n",
      "| Volc Engine Maas | This notebook provides you with a guide on how to get started with vo... |\n",
      "| Chat Writer | This notebook provides a quick overview for getting started with Writ... |\n",
      "| xAI | This page will help you get started with xAI chat models. For detaile... |\n",
      "| Xinference | Xinference is a powerful and versatile library designed to serve LLMs, |\n",
      "| YandexGPT | This notebook goes over how to use Langchain with YandexGPT chat mode... |\n",
      "| ChatYI | This will help you get started with Yi chat models. For detailed docu... |\n",
      "| Yuan2.0 | This notebook shows how to use YUAN2 API in LangChain with the langch... |\n",
      "| ZHIPU AI | This notebook shows how to use ZHIPU AI API in LangChain with the lan... |\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 获取当前 notebook 的工作目录（通常是 .ipynb 所在目录）\n",
    "notebook_dir = os.getcwd()\n",
    "target_path = os.path.abspath(os.path.join(notebook_dir, \"../\"))\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "from src.ingest.parsers.langchain_recursive_url import (\n",
    "    langchain_recursive_url_extractor,\n",
    "    langchain_recursive_url_metadata_extractor,\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://python.langchain.com/docs/integrations/chat/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "metadata = langchain_recursive_url_metadata_extractor(\n",
    "    raw_html=response.text,\n",
    "    url=url,\n",
    "    response=response,\n",
    "    type=\"doc\",\n",
    "    lang=\"python\",\n",
    ")\n",
    "print(metadata)\n",
    "doc = langchain_recursive_url_extractor(soup)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f15a8e",
   "metadata": {},
   "source": [
    "### LangChain Code Loader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d72f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'source': 'https://python.langchain.com/api_reference/_modules/langchain/agents/conversational_chat/output_parser.html', 'title': 'langchain.agents.conversational_chat.output_parser — 🦜🔗 LangChain  documentation', 'type': 'code', 'lang': 'python'}\n",
      "# Source code for langchain.agents.conversational_chat.output_parser\n",
      "\n",
      "```\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "from typing import Union\n",
      "\n",
      "from langchain_core.agents import AgentAction, AgentFinish\n",
      "from langchain_core.exceptions import OutputParserException\n",
      "from langchain_core.utils.json import parse_json_markdown\n",
      "\n",
      "from langchain.agents import AgentOutputParser\n",
      "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
      "\n",
      "# Define a class that parses output for conversational agents\n",
      "\n",
      "class ConvoOutputParser(AgentOutputParser):\n",
      "    \"\"\"Output parser for the conversational agent.\"\"\"\n",
      "\n",
      "    format_instructions: str = FORMAT_INSTRUCTIONS\n",
      "    \"\"\"Default formatting instructions\"\"\"\n",
      "\n",
      "    def get_format_instructions(self) -> str:\n",
      "        \"\"\"Returns formatting instructions for the given output parser.\"\"\"\n",
      "        return self.format_instructions\n",
      "\n",
      "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
      "        \"\"\"Attempts to parse the given text into an AgentAction or AgentFinish.\n",
      "\n",
      "        Raises:\n",
      "             OutputParserException if parsing fails.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            # Attempt to parse the text into a structured format (assumed to be JSON\n",
      "            # stored as markdown)\n",
      "            response = parse_json_markdown(text)\n",
      "\n",
      "            # If the response contains an 'action' and 'action_input'\n",
      "            if \"action\" in response and \"action_input\" in response:\n",
      "                action, action_input = response[\"action\"], response[\"action_input\"]\n",
      "\n",
      "                # If the action indicates a final answer, return an AgentFinish\n",
      "                if action == \"Final Answer\":\n",
      "                    return AgentFinish({\"output\": action_input}, text)\n",
      "                # Otherwise, return an AgentAction with the specified action and\n",
      "                # input\n",
      "                return AgentAction(action, action_input, text)\n",
      "            # If the necessary keys aren't present in the response, raise an\n",
      "            # exception\n",
      "            msg = f\"Missing 'action' or 'action_input' in LLM output: {text}\"\n",
      "            raise OutputParserException(msg)\n",
      "        except Exception as e:\n",
      "            # If any other exception is raised during parsing, also raise an\n",
      "            # OutputParserException\n",
      "            msg = f\"Could not parse LLM output: {text}\"\n",
      "            raise OutputParserException(msg) from e\n",
      "\n",
      "    @property\n",
      "    def _type(self) -> str:\n",
      "        return \"conversational_chat\"\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 获取当前 notebook 的工作目录（通常是 .ipynb 所在目录）\n",
    "notebook_dir = os.getcwd()\n",
    "target_path = os.path.abspath(os.path.join(notebook_dir, \"../\"))\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "from src.ingest.parsers.langchain_recursive_url import (\n",
    "    langchain_recursive_url_extractor,\n",
    "    langchain_recursive_url_metadata_extractor,\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://python.langchain.com/api_reference/_modules/langchain/agents/conversational_chat/output_parser.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "metadata = langchain_recursive_url_metadata_extractor(\n",
    "    raw_html=response.text,\n",
    "    url=url,\n",
    "    response=response,\n",
    "    type=\"code\",\n",
    "    lang=\"python\",\n",
    ")\n",
    "print(metadata)\n",
    "doc = langchain_recursive_url_extractor(soup)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e99888",
   "metadata": {},
   "source": [
    "### Langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaa8c9",
   "metadata": {},
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e1f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53212877",
   "metadata": {},
   "source": [
    "api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07ceb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c36f6548",
   "metadata": {},
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1e7d3",
   "metadata": {},
   "source": [
    "## Shadcn UI test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8fe49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a7821a5",
   "metadata": {},
   "source": [
    "## Tailwind CSS test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33785c",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7aa3a",
   "metadata": {},
   "source": [
    "## Prisma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
